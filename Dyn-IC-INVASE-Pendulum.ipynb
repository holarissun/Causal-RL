{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# import argparse\n",
    "from data_generation import generate_data\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import initpath_alg\n",
    "#initpath_alg.init_sys_path()\n",
    "import utilmlab\n",
    "import data_loader_mlab\n",
    "\n",
    "\n",
    "def array2str(a):\n",
    "    s = ''\n",
    "    for idx, el in enumerate(a):\n",
    "        s += (' ' if idx > 0 else '') + '{:0.3f}'.format(el)\n",
    "    return s\n",
    "\n",
    "\n",
    "def one_hot_encoder(a):\n",
    "    n_values = np.max(a) + 1\n",
    "    return np.eye(n_values)[a]\n",
    "\n",
    "\n",
    "def load_create_data(\n",
    "        data_type,\n",
    "        data_out,\n",
    "        is_logging_enabled=True,\n",
    "        fn_csv=None,\n",
    "        label_nm=None):\n",
    "\n",
    "    df_train, df_test, dset = None, None, None\n",
    "    features = None\n",
    "    if data_type in data_loader_mlab.get_available_datasets() + ['show'] \\\n",
    "       or fn_csv is not None:\n",
    "        if fn_csv is not None:\n",
    "            rval, dset = data_loader_mlab.load_dataset_from_csv(\n",
    "                logger, fn_csv, label_nm)\n",
    "        else:\n",
    "            rval, dset = data_loader_mlab.get_dataset(data_type)\n",
    "        assert rval == 0\n",
    "        data_loader_mlab.dataset_log_properties(logger, dset)\n",
    "        if is_logging_enabled:\n",
    "            logger.info('warning no seed')\n",
    "        df = dset['df']\n",
    "        features = dset['features']\n",
    "        labels = dset['targets']\n",
    "        nsample = len(df)\n",
    "        train_ratio = 0.8\n",
    "        idx = np.random.permutation(nsample)\n",
    "        ntrain = int(nsample * train_ratio)\n",
    "        df_train = df.iloc[idx[:ntrain]]\n",
    "        df_test = df.iloc[idx[ntrain:]]\n",
    "\n",
    "        col_drop = utilmlab.col_with_nan(df)\n",
    "        if is_logging_enabled and len(col_drop):\n",
    "            print('warning: dropping features {}'\n",
    "                  ', contains nan'.format(col_drop))\n",
    "            time.sleep(2)\n",
    "\n",
    "        features = [el for el in features if el not in col_drop]\n",
    "\n",
    "        x_train = df_train[features].values\n",
    "        y_train = df_train[labels].values\n",
    "        x_test = df_test[features].values\n",
    "        y_test = df_test[labels].values\n",
    "\n",
    "        g_train, g_test = None, None\n",
    "\n",
    "        y_train = one_hot_encoder(np.ravel(y_train))\n",
    "        y_test = one_hot_encoder(np.ravel(y_test))\n",
    "        if is_logging_enabled:\n",
    "            logger.info('y: train:{} test:{}'.format(\n",
    "                set(np.ravel(y_train)), set(np.ravel(y_test))))\n",
    "    else:\n",
    "        x_train, y_train, g_train = generate_data(\n",
    "            n=train_N, data_type=data_type, seed=train_seed, out=data_out, x_dim = X_DIM)\n",
    "        x_test,  y_test,  g_test = generate_data(\n",
    "            n=test_N,  data_type=data_type, seed=test_seed,  out=data_out, x_dim = X_DIM)\n",
    "    if is_logging_enabled:\n",
    "        logger.info('{} {} {} {}'.format(\n",
    "            x_train.shape,\n",
    "            y_train.shape,\n",
    "            x_test.shape,\n",
    "            y_test.shape))\n",
    "    return x_train, y_train, g_train, x_test, y_test, \\\n",
    "        g_test, df_train, df_test, dset, features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import embed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Generator (Actor) in PyTorch\n",
    "class INVASE_Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(INVASE_Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 100)\n",
    "        self.l2 = nn.Linear(100, 100)\n",
    "        self.l3 = nn.Linear(100, action_dim)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        \n",
    "        a = F.selu(self.l1(sa))\n",
    "        a = F.selu(self.l2(a))\n",
    "        return torch.sigmoid(self.l3(a))\n",
    "        \n",
    "# Discriminator (Critic) in PyTorch    \n",
    "# Critic in INVASE is a classifier that provide return signal\n",
    "class INVASE_Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(INVASE_Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 200)\n",
    "        #self.bn1 = nn.BatchNorm1d(num_features=200)\n",
    "        self.l2 = nn.Linear(200, 200)\n",
    "        #self.bn2 = nn.BatchNorm1d(num_features=200)\n",
    "        self.l3 = nn.Linear(200, state_dim)\n",
    "\n",
    "\n",
    "    def forward(self, state, action, mask):\n",
    "        #sa = torch.cat([state, action], 1)\n",
    "        sa = torch.cat([state, mask* action],1)\n",
    "        \n",
    "        #q1 = F.selu(self.bn1(self.l1(sa)))\n",
    "        #q1 = F.selu(self.bn2(self.l2(q1)))\n",
    "        q1 = F.selu(self.l1(sa))\n",
    "        q1 = F.selu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        return q1 # prob, actually the binary classification result with softmax activation (logits)\n",
    "    \n",
    "# Valuefunction (Baseline) in PyTorch   \n",
    "# Valuefunction in INVASE is a classifier that provide return signal\n",
    "class INVASE_Baseline(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(INVASE_Baseline, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 200)\n",
    "        #self.bn1 = nn.BatchNorm1d(num_features=200)\n",
    "        self.l2 = nn.Linear(200, 200)\n",
    "        #self.bn2 = nn.BatchNorm1d(num_features=200)\n",
    "        self.l3 = nn.Linear(200, state_dim)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        #sa = state\n",
    "\n",
    "        q1 = F.selu(self.l1(sa))\n",
    "        q1 = F.selu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        return q1 # prob, actually the binary classification result with softmax activation (logits)    \n",
    "\n",
    "\n",
    "class PVS():\n",
    "    # 1. Initialization\n",
    "    '''\n",
    "    x_train: training samples\n",
    "    data_type: Syn1 to Syn 6\n",
    "    '''\n",
    "    def __init__(self, xs_train, data_type, nepoch, is_logging_enabled=True, thres = 0.5):\n",
    "        self.is_logging_enabled = is_logging_enabled\n",
    "        self.latent_dim1 = 100      # Dimension of actor (generator) network\n",
    "        self.latent_dim2 = 200      # Dimension of critic (discriminator) network\n",
    "        \n",
    "        self.batch_size = min(1000, xs_train.shape[0])      # Batch size\n",
    "        self.epochs = nepoch        # Epoch size (large epoch is needed due to the policy gradient framework)\n",
    "        self.lamda = 1.0           # Hyper-parameter for the number of selected features \n",
    "        self.thres = thres\n",
    "        '''lamda is number of selected features? is it the coefficient?'''\n",
    "        \n",
    "        \n",
    "        self.input_shape_state = xs_train.shape[1]     # state dimension\n",
    "        self.input_shape_action = xa_train.shape[1]    # action dimension\n",
    "        logger.info('input shape: {}'.format(self.input_shape_state))\n",
    "        \n",
    "        # Actionvation. (For Syn1 and 2, relu, others, selu)\n",
    "        self.activation = 'relu' if data_type in ['Syn1','Syn2'] else 'selu'       \n",
    "        \n",
    "        \n",
    "        self.generator = INVASE_Actor(state_dim=self.input_shape_state, action_dim = self.input_shape_action)\n",
    "        self.discriminator = INVASE_Critic(state_dim=self.input_shape_state, action_dim = self.input_shape_action)\n",
    "        self.valfunction = INVASE_Baseline(state_dim=self.input_shape_state, action_dim = self.input_shape_action)\n",
    "        \n",
    "        \n",
    "        self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=1e-4)#,weight_decay=1e-3)\n",
    "        self.discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=1e-4)#,weight_decay=1e-3)\n",
    "        self.valfunction_optimizer = torch.optim.Adam(self.valfunction.parameters(), lr=1e-4)#,weight_decay=1e-3)\n",
    "        \n",
    "    def my_loss(self, y_true, y_pred,lmd, Thr):\n",
    "        # dimension of the features\n",
    "        \n",
    "        '''\n",
    "        sel_prob: the mask generated by bernulli sampler [bs, d]\n",
    "        dis_prob: prediction of the critic               [bs, state_dim]\n",
    "        val_prob: prediction of the baseline model       [bs, state_dim]\n",
    "        y_batch: batch of y_train                        [bs, state_dim]\n",
    "        all of those variables are 'placeholders'\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        d = y_pred.shape[1]        \n",
    "        \n",
    "        # Put all three in y_true \n",
    "        # 1. selected probability\n",
    "        sel_prob = y_true[:,:d] # bs x d\n",
    "        # 2. discriminator output\n",
    "        dis_prob = y_true[:,d:(d+self.input_shape_state)] # bs x 2\n",
    "        # 3. valfunction output\n",
    "        val_prob = y_true[:,(d+self.input_shape_state):(d+self.input_shape_state*2)] # bs x 2\n",
    "        # 4. ground truth\n",
    "        y_final = y_true[:,(d+self.input_shape_state*2):] # bs x 2\n",
    "        \n",
    "        # A1. Compute the rewards of the actor network\n",
    "        #embed()\n",
    "        Reward1 = torch.norm(y_final - dis_prob, p=2, dim=1)  \n",
    "\n",
    "        # A2. Compute the rewards of the actor network\n",
    "        Reward2 = torch.norm(y_final - val_prob, p=2, dim=1)  \n",
    "\n",
    "        # Difference is the rewards\n",
    "        Reward =Reward2 -  Reward1\n",
    "\n",
    "        # B. Policy gradient loss computation. \n",
    "        loss1 = Reward * torch.sum(sel_prob * torch.log(y_pred + 1e-8) + (1-sel_prob) * torch.log(1-y_pred + 1e-8), axis = 1) - lmd *torch.mean( torch.abs(y_pred-Thr), axis = 1)\n",
    "        \n",
    "        # C. Maximize the loss1\n",
    "        loss = torch.mean(-loss1)\n",
    "        #embed()\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def Sample_M(self, gen_prob):\n",
    "        # Shape of the selection probability\n",
    "        n = gen_prob.shape[0]\n",
    "        d = gen_prob.shape[1]\n",
    "        # Sampling\n",
    "        samples = np.random.binomial(1, gen_prob, (n,d))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    #%% Training procedure\n",
    "    def train(self, xs_train, xa_train, y_train, lmd, thr):\n",
    "\n",
    "        # For each epoch (actually iterations)\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            #%% Train Discriminator\n",
    "            # Select a random batch of samples\n",
    "            idx = np.random.randint(0, xs_train.shape[0], self.batch_size)\n",
    "            xs_batch = torch.as_tensor(xs_train[idx,:]).float()\n",
    "            xa_batch = torch.as_tensor(xa_train[idx,:]).float()\n",
    "            y_batch = torch.as_tensor(y_train[idx,:]).float() \n",
    "            # y_batch = torch.as_tensor(np.argmax(y_train[idx,:],1)).long()\n",
    "            \n",
    "            # Generate a batch of probabilities of feature selection\n",
    "            gen_prob = self.generator(xs_batch, xa_batch).cpu().detach().numpy()\n",
    "            # Sampling the features based on the generated probability\n",
    "            sel_prob = self.Sample_M(gen_prob)\n",
    "            '''sel_prob is the mask'''\n",
    "            \n",
    "            # Compute the prediction of the critic based on the sampled features (used for generator training)\n",
    "            dis_prob = self.discriminator(xs_batch, xa_batch, torch.as_tensor(sel_prob).float())\n",
    "            \n",
    "            # Train the discriminator\n",
    "            loss_func_c = nn.MSELoss()\n",
    "            self.discriminator_optimizer.zero_grad()\n",
    "            critic_loss = loss_func_c(dis_prob, y_batch)\n",
    "            critic_loss.backward()\n",
    "            self.discriminator_optimizer.step()\n",
    "\n",
    "            #%% Train Valud function\n",
    "\n",
    "            # Compute the prediction of the baseline based on the sampled features (used for generator training)\n",
    "            val_prob = self.valfunction(xs_batch, xa_batch)#.cpu().detach().numpy()\n",
    "            \n",
    "            # Train the baseline model\n",
    "            #v_loss = self.valfunction.train_on_batch(x_batch, y_batch)\n",
    "            loss_func_v = nn.MSELoss()\n",
    "            self.valfunction_optimizer.zero_grad()\n",
    "            value_loss = loss_func_v(val_prob, y_batch)\n",
    "            value_loss.backward()\n",
    "            self.valfunction_optimizer.step()\n",
    "            \n",
    "            \n",
    "            #%% Train Generator\n",
    "            # Use three things as the y_true: sel_prob, dis_prob, and ground truth (y_batch)\n",
    "            '''\n",
    "            sel_prob: the mask generated by bernulli sampler [bs, d]\n",
    "            dis_prob: prediction of the critic               [bs, state_dim]\n",
    "            val_prob: prediction of the baseline model       [bs, state_dim]\n",
    "            y_batch: batch of y_train                        [bs, state_dim]\n",
    "            all of those variables are 'placeholders'\n",
    "            '''\n",
    "            \n",
    "            y_batch_final = torch.as_tensor(np.concatenate( (sel_prob, torch.as_tensor(dis_prob).cpu().detach().numpy(), torch.as_tensor(val_prob).cpu().detach().numpy(), y_train[idx,:]), axis = 1 ))\n",
    "            # Train the generator\n",
    "            \n",
    "            actor_pred = self.generator(xs_batch,xa_batch)\n",
    "            self.generator_optimizer.zero_grad()\n",
    "            actor_loss = self.my_loss(y_batch_final,actor_pred,lmd,Thr)\n",
    "            actor_loss.backward()\n",
    "            self.generator_optimizer.step()\n",
    "            \n",
    "            #%% Plot the progress\n",
    "            dialog = 'Epoch: ' + '{:6d}'.format(epoch) + ', d_loss (Acc)): '\n",
    "            dialog += '{:0.3f}'.format(critic_loss) + ', v_loss (Acc): '\n",
    "            dialog += '{:0.3f}'.format(value_loss) + ', g_loss: ' + '{:+6.4f}'.format(actor_loss)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                logger.info('{}'.format(dialog))\n",
    "    \n",
    "    #%% Selected Features        \n",
    "    def output(self, xs_train, xa_train):\n",
    "        \n",
    "        gen_prob = self.generator(xs_train, xa_train).cpu().detach().numpy()\n",
    "        \n",
    "        return np.asarray(gen_prob)\n",
    "     \n",
    "    #%% Prediction Results \n",
    "    def get_prediction(self, xs, xa, m_train):\n",
    "        \n",
    "        val_prediction = self.valfunction(xs,xa).cpu().detach().numpy()\n",
    "        \n",
    "        dis_prediction = self.discriminator(xs,xa, m_train).cpu().detach().numpy()\n",
    "        \n",
    "        return np.asarray(val_prediction), np.asarray(dis_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " now evaluating: \n",
      "        Pendulum-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsun/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1521.553\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1538.150\n",
      "---------------------------------------\n",
      "Total T: 200 Episode Num: 1 Episode T: 200 Reward: -1019.088\n",
      "Total T: 400 Episode Num: 2 Episode T: 200 Reward: -1069.087\n",
      "Total T: 600 Episode Num: 3 Episode T: 200 Reward: -1519.138\n",
      "Total T: 800 Episode Num: 4 Episode T: 200 Reward: -1278.598\n",
      "Total T: 1000 Episode Num: 5 Episode T: 200 Reward: -862.967\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1505.696\n",
      "---------------------------------------\n",
      "recent Evaluation: -1505.6958268289775\n",
      "Total T: 1200 Episode Num: 6 Episode T: 200 Reward: -1063.940\n",
      "Total T: 1400 Episode Num: 7 Episode T: 200 Reward: -1058.628\n",
      "Total T: 1600 Episode Num: 8 Episode T: 200 Reward: -867.111\n",
      "Total T: 1800 Episode Num: 9 Episode T: 200 Reward: -953.699\n",
      "Total T: 2000 Episode Num: 10 Episode T: 200 Reward: -1182.579\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1615.009\n",
      "---------------------------------------\n",
      "recent Evaluation: -1615.0094331192167\n",
      "Total T: 2200 Episode Num: 11 Episode T: 200 Reward: -1300.549\n",
      "Total T: 2400 Episode Num: 12 Episode T: 200 Reward: -1299.592\n",
      "Total T: 2600 Episode Num: 13 Episode T: 200 Reward: -1635.778\n",
      "Total T: 2800 Episode Num: 14 Episode T: 200 Reward: -862.535\n",
      "Total T: 3000 Episode Num: 15 Episode T: 200 Reward: -916.646\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1491.645\n",
      "---------------------------------------\n",
      "recent Evaluation: -1491.6445002133773\n",
      "Total T: 3200 Episode Num: 16 Episode T: 200 Reward: -1192.429\n",
      "Total T: 3400 Episode Num: 17 Episode T: 200 Reward: -852.236\n",
      "Total T: 3600 Episode Num: 18 Episode T: 200 Reward: -855.145\n",
      "Total T: 3800 Episode Num: 19 Episode T: 200 Reward: -1609.023\n",
      "Total T: 4000 Episode Num: 20 Episode T: 200 Reward: -1180.780\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1550.012\n",
      "---------------------------------------\n",
      "recent Evaluation: -1550.0120442682705\n",
      "Total T: 4200 Episode Num: 21 Episode T: 200 Reward: -1080.042\n",
      "Total T: 4400 Episode Num: 22 Episode T: 200 Reward: -957.277\n",
      "Total T: 4600 Episode Num: 23 Episode T: 200 Reward: -1551.458\n",
      "Total T: 4800 Episode Num: 24 Episode T: 200 Reward: -1158.758\n",
      "Total T: 5000 Episode Num: 25 Episode T: 200 Reward: -1080.512\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1417.532\n",
      "---------------------------------------\n",
      "recent Evaluation: -1417.5317517083809\n",
      "Total T: 5200 Episode Num: 26 Episode T: 200 Reward: -1634.597\n",
      "Total T: 5400 Episode Num: 27 Episode T: 200 Reward: -1362.287\n",
      "Total T: 5600 Episode Num: 28 Episode T: 200 Reward: -1076.555\n",
      "Total T: 5800 Episode Num: 29 Episode T: 200 Reward: -1428.410\n",
      "Total T: 6000 Episode Num: 30 Episode T: 200 Reward: -1470.622\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1531.693\n",
      "---------------------------------------\n",
      "recent Evaluation: -1531.6933616958745\n",
      "Total T: 6200 Episode Num: 31 Episode T: 200 Reward: -945.057\n",
      "Total T: 6400 Episode Num: 32 Episode T: 200 Reward: -967.847\n",
      "Total T: 6600 Episode Num: 33 Episode T: 200 Reward: -1628.260\n",
      "Total T: 6800 Episode Num: 34 Episode T: 200 Reward: -1458.440\n",
      "Total T: 7000 Episode Num: 35 Episode T: 200 Reward: -1037.161\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1433.838\n",
      "---------------------------------------\n",
      "recent Evaluation: -1433.8384048287976\n",
      "Total T: 7200 Episode Num: 36 Episode T: 200 Reward: -1074.976\n",
      "Total T: 7400 Episode Num: 37 Episode T: 200 Reward: -973.406\n",
      "Total T: 7600 Episode Num: 38 Episode T: 200 Reward: -1192.239\n",
      "Total T: 7800 Episode Num: 39 Episode T: 200 Reward: -966.012\n",
      "Total T: 8000 Episode Num: 40 Episode T: 200 Reward: -892.284\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1410.435\n",
      "---------------------------------------\n",
      "recent Evaluation: -1410.435455532283\n",
      "Total T: 8200 Episode Num: 41 Episode T: 200 Reward: -1064.610\n",
      "Total T: 8400 Episode Num: 42 Episode T: 200 Reward: -965.545\n",
      "Total T: 8600 Episode Num: 43 Episode T: 200 Reward: -945.503\n",
      "Total T: 8800 Episode Num: 44 Episode T: 200 Reward: -1060.332\n",
      "Total T: 9000 Episode Num: 45 Episode T: 200 Reward: -1165.499\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1450.209\n",
      "---------------------------------------\n",
      "recent Evaluation: -1450.2085953342414\n",
      "Total T: 9200 Episode Num: 46 Episode T: 200 Reward: -1321.026\n",
      "Total T: 9400 Episode Num: 47 Episode T: 200 Reward: -873.066\n",
      "Total T: 9600 Episode Num: 48 Episode T: 200 Reward: -1611.879\n",
      "Total T: 9800 Episode Num: 49 Episode T: 200 Reward: -975.681\n",
      "Total T: 10000 Episode Num: 50 Episode T: 200 Reward: -1706.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invase: Syn5 300 . Y\n",
      "invase: Syn1 2500 . Y\n",
      "input shape: 3\n",
      "Epoch:      0, d_loss (Acc)): 0.165, v_loss (Acc): 0.200, g_loss: +4.7825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1401.752\n",
      "---------------------------------------\n",
      "recent Evaluation: -1401.7515668451001\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "(5000, 3) (5000, 101) (5000, 3) (5000, 3) (5000, 101) (5000, 3) (5000, 101)\n",
      "start training......\n",
      "now at training epoch number 0 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 60.7%  std: 48.8%\n",
      "FDR mean: 98.7%  std: 1.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.016, v_loss (Acc): 0.017, g_loss: +0.7524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.30659616  9.51890734]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "now at training epoch number 100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 80.4%  std: 39.7%\n",
      "FDR mean: 98.3%  std: 0.9%\n",
      "Epoch:      0, d_loss (Acc)): 0.015, v_loss (Acc): 0.011, g_loss: -1.5741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.37581565 8.79986379]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 91.3%  std: 28.2%\n",
      "FDR mean: 98.0%  std: 0.7%\n",
      "Epoch:      0, d_loss (Acc)): 0.013, v_loss (Acc): 0.010, g_loss: -1.0525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.33257468 8.66035849]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 96.6%  std: 18.1%\n",
      "FDR mean: 97.8%  std: 0.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.012, v_loss (Acc): 0.010, g_loss: -1.2542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.28255825 8.46588066]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 98.8%  std: 10.8%\n",
      "FDR mean: 97.6%  std: 0.4%\n",
      "Epoch:      0, d_loss (Acc)): 0.011, v_loss (Acc): 0.009, g_loss: -0.7170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.23175012 8.25304365]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 500 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.7%  std: 5.6%\n",
      "FDR mean: 97.4%  std: 0.4%\n",
      "Epoch:      0, d_loss (Acc)): 0.010, v_loss (Acc): 0.009, g_loss: -0.4100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.22430864 7.94217783]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 600 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 1.4%\n",
      "FDR mean: 97.0%  std: 0.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.009, v_loss (Acc): 0.008, g_loss: -0.1417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.14469684 7.53441851]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 700 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 96.4%  std: 0.7%\n",
      "Epoch:      0, d_loss (Acc)): 0.008, v_loss (Acc): 0.008, g_loss: -0.0085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.13214928 6.99076363]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 800 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 95.3%  std: 1.2%\n",
      "Epoch:      0, d_loss (Acc)): 0.007, v_loss (Acc): 0.007, g_loss: +0.2147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.11100704 6.26746739]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 900 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 93.1%  std: 2.4%\n",
      "Epoch:      0, d_loss (Acc)): 0.006, v_loss (Acc): 0.007, g_loss: +0.6591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.10006176 5.3932218 ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1000 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 88.3%  std: 5.7%\n",
      "Epoch:      0, d_loss (Acc)): 0.005, v_loss (Acc): 0.007, g_loss: +1.2279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.05915203 5.01605229]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 73.7%  std: 18.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.004, v_loss (Acc): 0.006, g_loss: +1.6010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.0092041  4.98577696]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 41.9%  std: 31.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.003, v_loss (Acc): 0.006, g_loss: +1.9878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.99314541 4.82267922]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 11.2%  std: 22.9%\n",
      "Epoch:      0, d_loss (Acc)): 0.002, v_loss (Acc): 0.005, g_loss: +2.3274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.95038472 4.49118178]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 2.3%  std: 10.9%\n",
      "Epoch:      0, d_loss (Acc)): 0.002, v_loss (Acc): 0.005, g_loss: +2.3986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.89691996 3.79109645]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1500 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.3%  std: 3.8%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.004, g_loss: +2.1182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.86118839 3.24079292]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1600 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 1.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.004, g_loss: +1.8602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.78757784 2.73463979]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1700 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.004, g_loss: +1.6634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.71433194 2.33133747]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1800 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +1.2671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.61928557 1.93652354]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1900 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +1.0533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.46127657 1.65008093]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2000 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.002, g_loss: +0.7888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.31972727 1.46870443]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.002, g_loss: +0.6398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.18960884 1.2793991 ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.4770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.98502023 1.13301086]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.001, g_loss: +0.3418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.79259525 1.01431751]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.59562277 0.94924869]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "PyTorch Version: elapsed time for Syn1: 11 feature, 10000 sample: [250.9811] sec.\n",
      "Total T: 10200 Episode Num: 51 Episode T: 200 Reward: -1440.603\n",
      "Total T: 10400 Episode Num: 52 Episode T: 200 Reward: -1202.390\n",
      "Total T: 10600 Episode Num: 53 Episode T: 200 Reward: -1578.900\n",
      "Total T: 10800 Episode Num: 54 Episode T: 200 Reward: -1812.212\n",
      "Total T: 11000 Episode Num: 55 Episode T: 200 Reward: -1828.275\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1549.953\n",
      "---------------------------------------\n",
      "recent Evaluation: -1549.9529251307645\n",
      "Total T: 11200 Episode Num: 56 Episode T: 200 Reward: -1306.107\n",
      "Total T: 11400 Episode Num: 57 Episode T: 200 Reward: -1390.792\n",
      "Total T: 11600 Episode Num: 58 Episode T: 200 Reward: -1502.800\n",
      "Total T: 11800 Episode Num: 59 Episode T: 200 Reward: -1510.539\n",
      "Total T: 12000 Episode Num: 60 Episode T: 200 Reward: -1189.604\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1258.704\n",
      "---------------------------------------\n",
      "recent Evaluation: -1258.704231754059\n",
      "Total T: 12200 Episode Num: 61 Episode T: 200 Reward: -1.403\n",
      "Total T: 12400 Episode Num: 62 Episode T: 200 Reward: -1248.667\n",
      "Total T: 12600 Episode Num: 63 Episode T: 200 Reward: -1304.716\n",
      "Total T: 12800 Episode Num: 64 Episode T: 200 Reward: -1032.361\n",
      "Total T: 13000 Episode Num: 65 Episode T: 200 Reward: -937.471\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1123.948\n",
      "---------------------------------------\n",
      "recent Evaluation: -1123.9478494073142\n",
      "Total T: 13200 Episode Num: 66 Episode T: 200 Reward: -1515.649\n",
      "Total T: 13400 Episode Num: 67 Episode T: 200 Reward: -532.818\n",
      "Total T: 13600 Episode Num: 68 Episode T: 200 Reward: -736.123\n",
      "Total T: 13800 Episode Num: 69 Episode T: 200 Reward: -131.545\n",
      "Total T: 14000 Episode Num: 70 Episode T: 200 Reward: -265.608\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -358.927\n",
      "---------------------------------------\n",
      "recent Evaluation: -358.92720371078957\n",
      "Total T: 14200 Episode Num: 71 Episode T: 200 Reward: -128.426\n",
      "Total T: 14400 Episode Num: 72 Episode T: 200 Reward: -131.174\n",
      "Total T: 14600 Episode Num: 73 Episode T: 200 Reward: -127.358\n",
      "Total T: 14800 Episode Num: 74 Episode T: 200 Reward: -1.607\n",
      "Total T: 15000 Episode Num: 75 Episode T: 200 Reward: -125.062\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -182.873\n",
      "---------------------------------------\n",
      "recent Evaluation: -182.87294743682986\n",
      "Total T: 15200 Episode Num: 76 Episode T: 200 Reward: -1.279\n",
      "Total T: 15400 Episode Num: 77 Episode T: 200 Reward: -275.947\n",
      "Total T: 15600 Episode Num: 78 Episode T: 200 Reward: -366.640\n",
      "Total T: 15800 Episode Num: 79 Episode T: 200 Reward: -124.996\n",
      "Total T: 16000 Episode Num: 80 Episode T: 200 Reward: -304.114\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -135.172\n",
      "---------------------------------------\n",
      "recent Evaluation: -135.17192615553716\n",
      "Total T: 16200 Episode Num: 81 Episode T: 200 Reward: -122.947\n",
      "Total T: 16400 Episode Num: 82 Episode T: 200 Reward: -126.966\n",
      "Total T: 16600 Episode Num: 83 Episode T: 200 Reward: -119.892\n",
      "Total T: 16800 Episode Num: 84 Episode T: 200 Reward: -125.116\n",
      "Total T: 17000 Episode Num: 85 Episode T: 200 Reward: -1.444\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -101.694\n",
      "---------------------------------------\n",
      "recent Evaluation: -101.69419949459466\n",
      "Total T: 17200 Episode Num: 86 Episode T: 200 Reward: -116.954\n",
      "Total T: 17400 Episode Num: 87 Episode T: 200 Reward: -1.427\n",
      "Total T: 17600 Episode Num: 88 Episode T: 200 Reward: -114.589\n",
      "Total T: 17800 Episode Num: 89 Episode T: 200 Reward: -250.245\n",
      "Total T: 18000 Episode Num: 90 Episode T: 200 Reward: -251.782\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -195.481\n",
      "---------------------------------------\n",
      "recent Evaluation: -195.48098750022578\n",
      "Total T: 18200 Episode Num: 91 Episode T: 200 Reward: -128.027\n",
      "Total T: 18400 Episode Num: 92 Episode T: 200 Reward: -244.634\n",
      "Total T: 18600 Episode Num: 93 Episode T: 200 Reward: -120.627\n",
      "Total T: 18800 Episode Num: 94 Episode T: 200 Reward: -245.802\n",
      "Total T: 19000 Episode Num: 95 Episode T: 200 Reward: -127.047\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -179.767\n",
      "---------------------------------------\n",
      "recent Evaluation: -179.76724215172516\n",
      "Total T: 19200 Episode Num: 96 Episode T: 200 Reward: -128.495\n",
      "Total T: 19400 Episode Num: 97 Episode T: 200 Reward: -1.721\n",
      "Total T: 19600 Episode Num: 98 Episode T: 200 Reward: -117.209\n",
      "Total T: 19800 Episode Num: 99 Episode T: 200 Reward: -246.624\n",
      "Total T: 20000 Episode Num: 100 Episode T: 200 Reward: -127.274\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -155.102\n",
      "---------------------------------------\n",
      "recent Evaluation: -155.10177220674365\n",
      "Total T: 20200 Episode Num: 101 Episode T: 200 Reward: -225.305\n",
      "Total T: 20400 Episode Num: 102 Episode T: 200 Reward: -236.636\n",
      "Total T: 20600 Episode Num: 103 Episode T: 200 Reward: -297.663\n",
      "Total T: 20800 Episode Num: 104 Episode T: 200 Reward: -122.750\n",
      "Total T: 21000 Episode Num: 105 Episode T: 200 Reward: -1.816\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -115.970\n",
      "---------------------------------------\n",
      "recent Evaluation: -115.96983757944892\n",
      "Total T: 21200 Episode Num: 106 Episode T: 200 Reward: -223.215\n",
      "Total T: 21400 Episode Num: 107 Episode T: 200 Reward: -1.537\n",
      "Total T: 21600 Episode Num: 108 Episode T: 200 Reward: -238.418\n",
      "Total T: 21800 Episode Num: 109 Episode T: 200 Reward: -120.747\n",
      "Total T: 22000 Episode Num: 110 Episode T: 200 Reward: -241.693\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -183.924\n",
      "---------------------------------------\n",
      "recent Evaluation: -183.9235310198425\n",
      "Total T: 22200 Episode Num: 111 Episode T: 200 Reward: -239.839\n",
      "Total T: 22400 Episode Num: 112 Episode T: 200 Reward: -335.486\n",
      "Total T: 22600 Episode Num: 113 Episode T: 200 Reward: -124.645\n",
      "Total T: 22800 Episode Num: 114 Episode T: 200 Reward: -125.200\n",
      "Total T: 23000 Episode Num: 115 Episode T: 200 Reward: -243.682\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -171.292\n",
      "---------------------------------------\n",
      "recent Evaluation: -171.2924794204518\n",
      "Total T: 23200 Episode Num: 116 Episode T: 200 Reward: -119.505\n",
      "Total T: 23400 Episode Num: 117 Episode T: 200 Reward: -123.423\n",
      "Total T: 23600 Episode Num: 118 Episode T: 200 Reward: -125.579\n",
      "Total T: 23800 Episode Num: 119 Episode T: 200 Reward: -116.431\n",
      "Total T: 24000 Episode Num: 120 Episode T: 200 Reward: -249.217\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -158.535\n",
      "---------------------------------------\n",
      "recent Evaluation: -158.53496291410403\n",
      "Total T: 24200 Episode Num: 121 Episode T: 200 Reward: -236.191\n",
      "Total T: 24400 Episode Num: 122 Episode T: 200 Reward: -359.846\n",
      "Total T: 24600 Episode Num: 123 Episode T: 200 Reward: -116.596\n",
      "Total T: 24800 Episode Num: 124 Episode T: 200 Reward: -116.848\n",
      "Total T: 25000 Episode Num: 125 Episode T: 200 Reward: -246.713\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -163.887\n",
      "---------------------------------------\n",
      "recent Evaluation: -163.88655318420172\n",
      "Total T: 25200 Episode Num: 126 Episode T: 200 Reward: -0.610\n",
      "Total T: 25400 Episode Num: 127 Episode T: 200 Reward: -1.338\n",
      "Total T: 25600 Episode Num: 128 Episode T: 200 Reward: -123.172\n",
      "Total T: 25800 Episode Num: 129 Episode T: 200 Reward: -120.032\n",
      "Total T: 26000 Episode Num: 130 Episode T: 200 Reward: -122.818\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -170.474\n",
      "---------------------------------------\n",
      "recent Evaluation: -170.47425843181307\n",
      "Total T: 26200 Episode Num: 131 Episode T: 200 Reward: -0.688\n",
      "Total T: 26400 Episode Num: 132 Episode T: 200 Reward: -117.111\n",
      "Total T: 26600 Episode Num: 133 Episode T: 200 Reward: -116.157\n",
      "Total T: 26800 Episode Num: 134 Episode T: 200 Reward: -120.848\n",
      "Total T: 27000 Episode Num: 135 Episode T: 200 Reward: -116.777\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -163.692\n",
      "---------------------------------------\n",
      "recent Evaluation: -163.69208988934048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 27200 Episode Num: 136 Episode T: 200 Reward: -121.844\n",
      "Total T: 27400 Episode Num: 137 Episode T: 200 Reward: -248.529\n",
      "Total T: 27600 Episode Num: 138 Episode T: 200 Reward: -230.310\n",
      "Total T: 27800 Episode Num: 139 Episode T: 200 Reward: -116.803\n",
      "Total T: 28000 Episode Num: 140 Episode T: 200 Reward: -118.463\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -174.509\n",
      "---------------------------------------\n",
      "recent Evaluation: -174.5094718660324\n",
      "Total T: 28200 Episode Num: 141 Episode T: 200 Reward: -317.794\n",
      "Total T: 28400 Episode Num: 142 Episode T: 200 Reward: -119.469\n",
      "Total T: 28600 Episode Num: 143 Episode T: 200 Reward: -360.110\n",
      "Total T: 28800 Episode Num: 144 Episode T: 200 Reward: -231.568\n",
      "Total T: 29000 Episode Num: 145 Episode T: 200 Reward: -298.432\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -175.396\n",
      "---------------------------------------\n",
      "recent Evaluation: -175.3958409419151\n",
      "Total T: 29200 Episode Num: 146 Episode T: 200 Reward: -125.221\n",
      "Total T: 29400 Episode Num: 147 Episode T: 200 Reward: -1.849\n",
      "Total T: 29600 Episode Num: 148 Episode T: 200 Reward: -2.401\n",
      "Total T: 29800 Episode Num: 149 Episode T: 200 Reward: -119.851\n",
      "Total T: 30000 Episode Num: 150 Episode T: 200 Reward: -118.682\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -114.427\n",
      "---------------------------------------\n",
      "recent Evaluation: -114.42682319898199\n",
      "Total T: 30200 Episode Num: 151 Episode T: 200 Reward: -115.290\n",
      "Total T: 30400 Episode Num: 152 Episode T: 200 Reward: -117.720\n",
      "Total T: 30600 Episode Num: 153 Episode T: 200 Reward: -120.064\n",
      "Total T: 30800 Episode Num: 154 Episode T: 200 Reward: -243.927\n",
      "Total T: 31000 Episode Num: 155 Episode T: 200 Reward: -224.987\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -139.634\n",
      "---------------------------------------\n",
      "recent Evaluation: -139.633720565895\n",
      "Total T: 31200 Episode Num: 156 Episode T: 200 Reward: -118.457\n",
      "Total T: 31400 Episode Num: 157 Episode T: 200 Reward: -226.157\n",
      "Total T: 31600 Episode Num: 158 Episode T: 200 Reward: -2.188\n",
      "Total T: 31800 Episode Num: 159 Episode T: 200 Reward: -1.743\n",
      "Total T: 32000 Episode Num: 160 Episode T: 200 Reward: -225.145\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.990\n",
      "---------------------------------------\n",
      "recent Evaluation: -132.99022176459428\n",
      "Total T: 32200 Episode Num: 161 Episode T: 200 Reward: -237.405\n",
      "Total T: 32400 Episode Num: 162 Episode T: 200 Reward: -124.747\n",
      "Total T: 32600 Episode Num: 163 Episode T: 200 Reward: -125.734\n",
      "Total T: 32800 Episode Num: 164 Episode T: 200 Reward: -116.454\n",
      "Total T: 33000 Episode Num: 165 Episode T: 200 Reward: -225.875\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -167.601\n",
      "---------------------------------------\n",
      "recent Evaluation: -167.60097723805288\n",
      "Total T: 33200 Episode Num: 166 Episode T: 200 Reward: -226.139\n",
      "Total T: 33400 Episode Num: 167 Episode T: 200 Reward: -119.950\n",
      "Total T: 33600 Episode Num: 168 Episode T: 200 Reward: -125.300\n",
      "Total T: 33800 Episode Num: 169 Episode T: 200 Reward: -122.932\n",
      "Total T: 34000 Episode Num: 170 Episode T: 200 Reward: -231.188\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -177.281\n",
      "---------------------------------------\n",
      "recent Evaluation: -177.2814327472725\n",
      "Total T: 34200 Episode Num: 171 Episode T: 200 Reward: -125.712\n",
      "Total T: 34400 Episode Num: 172 Episode T: 200 Reward: -123.566\n",
      "Total T: 34600 Episode Num: 173 Episode T: 200 Reward: -126.025\n",
      "Total T: 34800 Episode Num: 174 Episode T: 200 Reward: -114.586\n",
      "Total T: 35000 Episode Num: 175 Episode T: 200 Reward: -116.298\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -139.708\n",
      "---------------------------------------\n",
      "recent Evaluation: -139.70799603109361\n",
      "Total T: 35200 Episode Num: 176 Episode T: 200 Reward: -117.107\n",
      "Total T: 35400 Episode Num: 177 Episode T: 200 Reward: -225.278\n",
      "Total T: 35600 Episode Num: 178 Episode T: 200 Reward: -120.632\n",
      "Total T: 35800 Episode Num: 179 Episode T: 200 Reward: -127.387\n",
      "Total T: 36000 Episode Num: 180 Episode T: 200 Reward: -123.476\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -107.985\n",
      "---------------------------------------\n",
      "recent Evaluation: -107.98459783007578\n",
      "Total T: 36200 Episode Num: 181 Episode T: 200 Reward: -236.860\n",
      "Total T: 36400 Episode Num: 182 Episode T: 200 Reward: -1.611\n",
      "Total T: 36600 Episode Num: 183 Episode T: 200 Reward: -126.771\n",
      "Total T: 36800 Episode Num: 184 Episode T: 200 Reward: -119.693\n",
      "Total T: 37000 Episode Num: 185 Episode T: 200 Reward: -122.464\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -96.040\n",
      "---------------------------------------\n",
      "recent Evaluation: -96.03966054990242\n",
      "Total T: 37200 Episode Num: 186 Episode T: 200 Reward: -343.736\n",
      "Total T: 37400 Episode Num: 187 Episode T: 200 Reward: -230.895\n",
      "Total T: 37600 Episode Num: 188 Episode T: 200 Reward: -124.818\n",
      "Total T: 37800 Episode Num: 189 Episode T: 200 Reward: -129.642\n",
      "Total T: 38000 Episode Num: 190 Episode T: 200 Reward: -123.863\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -165.236\n",
      "---------------------------------------\n",
      "recent Evaluation: -165.23607549272236\n",
      "Total T: 38200 Episode Num: 191 Episode T: 200 Reward: -286.936\n",
      "Total T: 38400 Episode Num: 192 Episode T: 200 Reward: -234.004\n",
      "Total T: 38600 Episode Num: 193 Episode T: 200 Reward: -119.313\n",
      "Total T: 38800 Episode Num: 194 Episode T: 200 Reward: -1.417\n",
      "Total T: 39000 Episode Num: 195 Episode T: 200 Reward: -115.296\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -144.912\n",
      "---------------------------------------\n",
      "recent Evaluation: -144.91174785838078\n",
      "Total T: 39200 Episode Num: 196 Episode T: 200 Reward: -1.240\n",
      "Total T: 39400 Episode Num: 197 Episode T: 200 Reward: -367.217\n",
      "Total T: 39600 Episode Num: 198 Episode T: 200 Reward: -4.147\n",
      "Total T: 39800 Episode Num: 199 Episode T: 200 Reward: -116.727\n",
      "Total T: 40000 Episode Num: 200 Episode T: 200 Reward: -126.576\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -186.585\n",
      "---------------------------------------\n",
      "recent Evaluation: -186.5850485765527\n",
      "Total T: 40200 Episode Num: 201 Episode T: 200 Reward: -130.801\n",
      "Total T: 40400 Episode Num: 202 Episode T: 200 Reward: -128.511\n",
      "Total T: 40600 Episode Num: 203 Episode T: 200 Reward: -323.506\n",
      "Total T: 40800 Episode Num: 204 Episode T: 200 Reward: -125.208\n",
      "Total T: 41000 Episode Num: 205 Episode T: 200 Reward: -1.065\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -136.736\n",
      "---------------------------------------\n",
      "recent Evaluation: -136.73629049707336\n",
      "Total T: 41200 Episode Num: 206 Episode T: 200 Reward: -1.038\n",
      "Total T: 41400 Episode Num: 207 Episode T: 200 Reward: -119.284\n",
      "Total T: 41600 Episode Num: 208 Episode T: 200 Reward: -245.388\n",
      "Total T: 41800 Episode Num: 209 Episode T: 200 Reward: -125.345\n",
      "Total T: 42000 Episode Num: 210 Episode T: 200 Reward: -126.801\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -131.557\n",
      "---------------------------------------\n",
      "recent Evaluation: -131.55711447442417\n",
      "Total T: 42200 Episode Num: 211 Episode T: 200 Reward: -1.209\n",
      "Total T: 42400 Episode Num: 212 Episode T: 200 Reward: -116.165\n",
      "Total T: 42600 Episode Num: 213 Episode T: 200 Reward: -118.894\n",
      "Total T: 42800 Episode Num: 214 Episode T: 200 Reward: -225.888\n",
      "Total T: 43000 Episode Num: 215 Episode T: 200 Reward: -343.642\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -155.063\n",
      "---------------------------------------\n",
      "recent Evaluation: -155.06285192632927\n",
      "Total T: 43200 Episode Num: 216 Episode T: 200 Reward: -113.573\n",
      "Total T: 43400 Episode Num: 217 Episode T: 200 Reward: -121.027\n",
      "Total T: 43600 Episode Num: 218 Episode T: 200 Reward: -114.672\n",
      "Total T: 43800 Episode Num: 219 Episode T: 200 Reward: -1.110\n",
      "Total T: 44000 Episode Num: 220 Episode T: 200 Reward: -224.285\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -124.350\n",
      "---------------------------------------\n",
      "recent Evaluation: -124.35025416194237\n",
      "Total T: 44200 Episode Num: 221 Episode T: 200 Reward: -235.395\n",
      "Total T: 44400 Episode Num: 222 Episode T: 200 Reward: -117.040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 44600 Episode Num: 223 Episode T: 200 Reward: -126.215\n",
      "Total T: 44800 Episode Num: 224 Episode T: 200 Reward: -242.141\n",
      "Total T: 45000 Episode Num: 225 Episode T: 200 Reward: -237.585\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -110.244\n",
      "---------------------------------------\n",
      "recent Evaluation: -110.24372030902575\n",
      "Total T: 45200 Episode Num: 226 Episode T: 200 Reward: -113.963\n",
      "Total T: 45400 Episode Num: 227 Episode T: 200 Reward: -129.500\n",
      "Total T: 45600 Episode Num: 228 Episode T: 200 Reward: -120.482\n",
      "Total T: 45800 Episode Num: 229 Episode T: 200 Reward: -233.996\n",
      "Total T: 46000 Episode Num: 230 Episode T: 200 Reward: -224.928\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.038\n",
      "---------------------------------------\n",
      "recent Evaluation: -132.03788474813288\n",
      "Total T: 46200 Episode Num: 231 Episode T: 200 Reward: -129.426\n",
      "Total T: 46400 Episode Num: 232 Episode T: 200 Reward: -121.902\n",
      "Total T: 46600 Episode Num: 233 Episode T: 200 Reward: -118.383\n",
      "Total T: 46800 Episode Num: 234 Episode T: 200 Reward: -235.353\n",
      "Total T: 47000 Episode Num: 235 Episode T: 200 Reward: -241.199\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -165.155\n",
      "---------------------------------------\n",
      "recent Evaluation: -165.15477631659206\n",
      "Total T: 47200 Episode Num: 236 Episode T: 200 Reward: -118.977\n",
      "Total T: 47400 Episode Num: 237 Episode T: 200 Reward: -123.924\n",
      "Total T: 47600 Episode Num: 238 Episode T: 200 Reward: -0.676\n",
      "Total T: 47800 Episode Num: 239 Episode T: 200 Reward: -226.230\n",
      "Total T: 48000 Episode Num: 240 Episode T: 200 Reward: -117.010\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -166.540\n",
      "---------------------------------------\n",
      "recent Evaluation: -166.54011924827154\n",
      "Total T: 48200 Episode Num: 241 Episode T: 200 Reward: -235.441\n",
      "Total T: 48400 Episode Num: 242 Episode T: 200 Reward: -127.954\n",
      "Total T: 48600 Episode Num: 243 Episode T: 200 Reward: -1.452\n",
      "Total T: 48800 Episode Num: 244 Episode T: 200 Reward: -130.709\n",
      "Total T: 49000 Episode Num: 245 Episode T: 200 Reward: -0.957\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -130.243\n",
      "---------------------------------------\n",
      "recent Evaluation: -130.24340867905383\n",
      "Total T: 49200 Episode Num: 246 Episode T: 200 Reward: -125.545\n",
      "Total T: 49400 Episode Num: 247 Episode T: 200 Reward: -130.597\n",
      "Total T: 49600 Episode Num: 248 Episode T: 200 Reward: -130.206\n",
      "Total T: 49800 Episode Num: 249 Episode T: 200 Reward: -317.584\n",
      "Total T: 50000 Episode Num: 250 Episode T: 200 Reward: -320.341\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -152.152\n",
      "---------------------------------------\n",
      "recent Evaluation: -152.15183868719538\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1407.960\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1434.509\n",
      "---------------------------------------\n",
      "Total T: 200 Episode Num: 1 Episode T: 200 Reward: -1435.874\n",
      "Total T: 400 Episode Num: 2 Episode T: 200 Reward: -1414.211\n",
      "Total T: 600 Episode Num: 3 Episode T: 200 Reward: -862.882\n",
      "Total T: 800 Episode Num: 4 Episode T: 200 Reward: -1292.117\n",
      "Total T: 1000 Episode Num: 5 Episode T: 200 Reward: -1679.399\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1468.301\n",
      "---------------------------------------\n",
      "recent Evaluation: -1468.301102074403\n",
      "Total T: 1200 Episode Num: 6 Episode T: 200 Reward: -1323.445\n",
      "Total T: 1400 Episode Num: 7 Episode T: 200 Reward: -957.943\n",
      "Total T: 1600 Episode Num: 8 Episode T: 200 Reward: -1187.192\n",
      "Total T: 1800 Episode Num: 9 Episode T: 200 Reward: -1523.659\n",
      "Total T: 2000 Episode Num: 10 Episode T: 200 Reward: -1575.608\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1427.454\n",
      "---------------------------------------\n",
      "recent Evaluation: -1427.453877328257\n",
      "Total T: 2200 Episode Num: 11 Episode T: 200 Reward: -1332.270\n",
      "Total T: 2400 Episode Num: 12 Episode T: 200 Reward: -853.460\n",
      "Total T: 2600 Episode Num: 13 Episode T: 200 Reward: -1262.755\n",
      "Total T: 2800 Episode Num: 14 Episode T: 200 Reward: -1104.777\n",
      "Total T: 3000 Episode Num: 15 Episode T: 200 Reward: -868.651\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1417.653\n",
      "---------------------------------------\n",
      "recent Evaluation: -1417.6525844363266\n",
      "Total T: 3200 Episode Num: 16 Episode T: 200 Reward: -752.218\n",
      "Total T: 3400 Episode Num: 17 Episode T: 200 Reward: -1363.232\n",
      "Total T: 3600 Episode Num: 18 Episode T: 200 Reward: -1625.745\n",
      "Total T: 3800 Episode Num: 19 Episode T: 200 Reward: -987.980\n",
      "Total T: 4000 Episode Num: 20 Episode T: 200 Reward: -1800.867\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1373.388\n",
      "---------------------------------------\n",
      "recent Evaluation: -1373.3882068532782\n",
      "Total T: 4200 Episode Num: 21 Episode T: 200 Reward: -1280.559\n",
      "Total T: 4400 Episode Num: 22 Episode T: 200 Reward: -1128.265\n",
      "Total T: 4600 Episode Num: 23 Episode T: 200 Reward: -842.487\n",
      "Total T: 4800 Episode Num: 24 Episode T: 200 Reward: -1298.578\n",
      "Total T: 5000 Episode Num: 25 Episode T: 200 Reward: -1162.814\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1357.981\n",
      "---------------------------------------\n",
      "recent Evaluation: -1357.9813764380074\n",
      "Total T: 5200 Episode Num: 26 Episode T: 200 Reward: -1629.684\n",
      "Total T: 5400 Episode Num: 27 Episode T: 200 Reward: -1218.850\n",
      "Total T: 5600 Episode Num: 28 Episode T: 200 Reward: -879.213\n",
      "Total T: 5800 Episode Num: 29 Episode T: 200 Reward: -1553.870\n",
      "Total T: 6000 Episode Num: 30 Episode T: 200 Reward: -1024.173\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1448.523\n",
      "---------------------------------------\n",
      "recent Evaluation: -1448.523265998451\n",
      "Total T: 6200 Episode Num: 31 Episode T: 200 Reward: -1272.877\n",
      "Total T: 6400 Episode Num: 32 Episode T: 200 Reward: -976.404\n",
      "Total T: 6600 Episode Num: 33 Episode T: 200 Reward: -965.992\n",
      "Total T: 6800 Episode Num: 34 Episode T: 200 Reward: -1315.321\n",
      "Total T: 7000 Episode Num: 35 Episode T: 200 Reward: -1347.166\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1485.763\n",
      "---------------------------------------\n",
      "recent Evaluation: -1485.7632557199884\n",
      "Total T: 7200 Episode Num: 36 Episode T: 200 Reward: -1847.540\n",
      "Total T: 7400 Episode Num: 37 Episode T: 200 Reward: -1673.852\n",
      "Total T: 7600 Episode Num: 38 Episode T: 200 Reward: -1103.079\n",
      "Total T: 7800 Episode Num: 39 Episode T: 200 Reward: -1318.385\n",
      "Total T: 8000 Episode Num: 40 Episode T: 200 Reward: -1502.957\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1698.454\n",
      "---------------------------------------\n",
      "recent Evaluation: -1698.4535712454447\n",
      "Total T: 8200 Episode Num: 41 Episode T: 200 Reward: -1163.153\n",
      "Total T: 8400 Episode Num: 42 Episode T: 200 Reward: -1317.405\n",
      "Total T: 8600 Episode Num: 43 Episode T: 200 Reward: -893.765\n",
      "Total T: 8800 Episode Num: 44 Episode T: 200 Reward: -874.171\n",
      "Total T: 9000 Episode Num: 45 Episode T: 200 Reward: -967.516\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1367.502\n",
      "---------------------------------------\n",
      "recent Evaluation: -1367.5020192040279\n",
      "Total T: 9200 Episode Num: 46 Episode T: 200 Reward: -1330.232\n",
      "Total T: 9400 Episode Num: 47 Episode T: 200 Reward: -1098.933\n",
      "Total T: 9600 Episode Num: 48 Episode T: 200 Reward: -1488.539\n",
      "Total T: 9800 Episode Num: 49 Episode T: 200 Reward: -1177.397\n",
      "Total T: 10000 Episode Num: 50 Episode T: 200 Reward: -974.097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invase: Syn5 300 . Y\n",
      "invase: Syn1 2500 . Y\n",
      "input shape: 3\n",
      "Epoch:      0, d_loss (Acc)): 0.172, v_loss (Acc): 0.197, g_loss: +3.6549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1413.927\n",
      "---------------------------------------\n",
      "recent Evaluation: -1413.9265831863352\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "(5000, 3) (5000, 101) (5000, 3) (5000, 3) (5000, 101) (5000, 3) (5000, 101)\n",
      "start training......\n",
      "now at training epoch number 0 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 53.0%  std: 49.9%\n",
      "FDR mean: 99.0%  std: 1.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.015, v_loss (Acc): 0.016, g_loss: +0.6734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.54894208 10.79465737]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "now at training epoch number 100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 81.6%  std: 38.7%\n",
      "FDR mean: 98.4%  std: 0.8%\n",
      "Epoch:      0, d_loss (Acc)): 0.012, v_loss (Acc): 0.009, g_loss: -1.6189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.99430559 8.70896077]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 93.7%  std: 24.3%\n",
      "FDR mean: 98.1%  std: 0.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.011, v_loss (Acc): 0.008, g_loss: -1.6676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.97038523 8.61154523]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 97.6%  std: 15.2%\n",
      "FDR mean: 98.0%  std: 0.4%\n",
      "Epoch:      0, d_loss (Acc)): 0.010, v_loss (Acc): 0.007, g_loss: -1.2112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.90915804 8.42932089]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.1%  std: 9.3%\n",
      "FDR mean: 97.8%  std: 0.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.009, v_loss (Acc): 0.007, g_loss: -1.0194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.84590697 8.28477244]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 500 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.7%  std: 5.8%\n",
      "FDR mean: 97.7%  std: 0.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.008, v_loss (Acc): 0.007, g_loss: -0.9591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.80572715 8.09496896]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 600 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.9%  std: 3.2%\n",
      "FDR mean: 97.4%  std: 0.4%\n",
      "Epoch:      0, d_loss (Acc)): 0.008, v_loss (Acc): 0.007, g_loss: -0.5849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.77496737 7.82781638]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 700 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 2.0%\n",
      "FDR mean: 96.9%  std: 0.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.007, v_loss (Acc): 0.006, g_loss: -0.2409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.72425183 7.46283956]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 800 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 96.0%  std: 0.9%\n",
      "Epoch:      0, d_loss (Acc)): 0.007, v_loss (Acc): 0.006, g_loss: -0.4366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.67125605 7.05749222]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 900 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 94.1%  std: 1.8%\n",
      "Epoch:      0, d_loss (Acc)): 0.006, v_loss (Acc): 0.006, g_loss: -0.2080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.61832132 6.48124077]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1000 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 89.4%  std: 5.2%\n",
      "Epoch:      0, d_loss (Acc)): 0.006, v_loss (Acc): 0.005, g_loss: -0.2196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.58351094 6.06840076]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 73.7%  std: 20.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.005, v_loss (Acc): 0.005, g_loss: +0.1451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.48221224 5.98983474]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 38.6%  std: 32.7%\n",
      "Epoch:      0, d_loss (Acc)): 0.004, v_loss (Acc): 0.004, g_loss: +0.4652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.41597683 5.59642426]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 10.9%  std: 23.2%\n",
      "Epoch:      0, d_loss (Acc)): 0.003, v_loss (Acc): 0.004, g_loss: +0.6013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.33116001 5.2626247 ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 1.8%  std: 10.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.003, v_loss (Acc): 0.004, g_loss: +0.8903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.22327391 4.24380055]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1500 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.2%  std: 3.6%\n",
      "Epoch:      0, d_loss (Acc)): 0.002, v_loss (Acc): 0.003, g_loss: +0.9775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.12039014 3.61371224]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1600 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 1.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.002, v_loss (Acc): 0.003, g_loss: +0.8985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.99066208 3.06292298]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1700 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +0.8747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.90997242 2.48028102]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1800 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +0.6861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.75939713 2.16129087]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1900 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.002, g_loss: +0.5938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.58987206 1.92442106]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2000 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.002, g_loss: +0.4898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.46583139 1.80542781]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.3879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.32805608 1.72988786]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.2854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.19757347 1.6890226 ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.001, g_loss: +0.2177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.10683049 1.6241501 ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.99321383 1.60977811]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "PyTorch Version: elapsed time for Syn1: 11 feature, 10000 sample: [248.5574] sec.\n",
      "Total T: 10200 Episode Num: 51 Episode T: 200 Reward: -1374.422\n",
      "Total T: 10400 Episode Num: 52 Episode T: 200 Reward: -1803.300\n",
      "Total T: 10600 Episode Num: 53 Episode T: 200 Reward: -1626.236\n",
      "Total T: 10800 Episode Num: 54 Episode T: 200 Reward: -1827.035\n",
      "Total T: 11000 Episode Num: 55 Episode T: 200 Reward: -1552.003\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1508.162\n",
      "---------------------------------------\n",
      "recent Evaluation: -1508.161904787845\n",
      "Total T: 11200 Episode Num: 56 Episode T: 200 Reward: -1475.327\n",
      "Total T: 11400 Episode Num: 57 Episode T: 200 Reward: -1385.428\n",
      "Total T: 11600 Episode Num: 58 Episode T: 200 Reward: -1511.358\n",
      "Total T: 11800 Episode Num: 59 Episode T: 200 Reward: -1511.440\n",
      "Total T: 12000 Episode Num: 60 Episode T: 200 Reward: -1537.156\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1377.080\n",
      "---------------------------------------\n",
      "recent Evaluation: -1377.0800873923276\n",
      "Total T: 12200 Episode Num: 61 Episode T: 200 Reward: -1150.357\n",
      "Total T: 12400 Episode Num: 62 Episode T: 200 Reward: -1220.084\n",
      "Total T: 12600 Episode Num: 63 Episode T: 200 Reward: -4.663\n",
      "Total T: 12800 Episode Num: 64 Episode T: 200 Reward: -1093.563\n",
      "Total T: 13000 Episode Num: 65 Episode T: 200 Reward: -947.991\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1033.382\n",
      "---------------------------------------\n",
      "recent Evaluation: -1033.3815247057614\n",
      "Total T: 13200 Episode Num: 66 Episode T: 200 Reward: -925.765\n",
      "Total T: 13400 Episode Num: 67 Episode T: 200 Reward: -133.576\n",
      "Total T: 13600 Episode Num: 68 Episode T: 200 Reward: -0.980\n",
      "Total T: 13800 Episode Num: 69 Episode T: 200 Reward: -1510.800\n",
      "Total T: 14000 Episode Num: 70 Episode T: 200 Reward: -134.072\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -378.234\n",
      "---------------------------------------\n",
      "recent Evaluation: -378.2343364363517\n",
      "Total T: 14200 Episode Num: 71 Episode T: 200 Reward: -129.716\n",
      "Total T: 14400 Episode Num: 72 Episode T: 200 Reward: -3.543\n",
      "Total T: 14600 Episode Num: 73 Episode T: 200 Reward: -131.361\n",
      "Total T: 14800 Episode Num: 74 Episode T: 200 Reward: -271.994\n",
      "Total T: 15000 Episode Num: 75 Episode T: 200 Reward: -402.895\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -285.583\n",
      "---------------------------------------\n",
      "recent Evaluation: -285.5829056869803\n",
      "Total T: 15200 Episode Num: 76 Episode T: 200 Reward: -2.154\n",
      "Total T: 15400 Episode Num: 77 Episode T: 200 Reward: -128.649\n",
      "Total T: 15600 Episode Num: 78 Episode T: 200 Reward: -133.019\n",
      "Total T: 15800 Episode Num: 79 Episode T: 200 Reward: -132.163\n",
      "Total T: 16000 Episode Num: 80 Episode T: 200 Reward: -245.064\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -169.274\n",
      "---------------------------------------\n",
      "recent Evaluation: -169.2739260956564\n",
      "Total T: 16200 Episode Num: 81 Episode T: 200 Reward: -2.045\n",
      "Total T: 16400 Episode Num: 82 Episode T: 200 Reward: -126.484\n",
      "Total T: 16600 Episode Num: 83 Episode T: 200 Reward: -384.901\n",
      "Total T: 16800 Episode Num: 84 Episode T: 200 Reward: -255.300\n",
      "Total T: 17000 Episode Num: 85 Episode T: 200 Reward: -236.938\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -109.811\n",
      "---------------------------------------\n",
      "recent Evaluation: -109.81058103464281\n",
      "Total T: 17200 Episode Num: 86 Episode T: 200 Reward: -0.804\n",
      "Total T: 17400 Episode Num: 87 Episode T: 200 Reward: -126.242\n",
      "Total T: 17600 Episode Num: 88 Episode T: 200 Reward: -124.858\n",
      "Total T: 17800 Episode Num: 89 Episode T: 200 Reward: -0.344\n",
      "Total T: 18000 Episode Num: 90 Episode T: 200 Reward: -249.671\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -134.267\n",
      "---------------------------------------\n",
      "recent Evaluation: -134.26667460269073\n",
      "Total T: 18200 Episode Num: 91 Episode T: 200 Reward: -116.045\n",
      "Total T: 18400 Episode Num: 92 Episode T: 200 Reward: -127.487\n",
      "Total T: 18600 Episode Num: 93 Episode T: 200 Reward: -345.101\n",
      "Total T: 18800 Episode Num: 94 Episode T: 200 Reward: -354.561\n",
      "Total T: 19000 Episode Num: 95 Episode T: 200 Reward: -122.382\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -112.624\n",
      "---------------------------------------\n",
      "recent Evaluation: -112.62400394063218\n",
      "Total T: 19200 Episode Num: 96 Episode T: 200 Reward: -322.932\n",
      "Total T: 19400 Episode Num: 97 Episode T: 200 Reward: -1.083\n",
      "Total T: 19600 Episode Num: 98 Episode T: 200 Reward: -115.060\n",
      "Total T: 19800 Episode Num: 99 Episode T: 200 Reward: -1.478\n",
      "Total T: 20000 Episode Num: 100 Episode T: 200 Reward: -247.162\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -157.779\n",
      "---------------------------------------\n",
      "recent Evaluation: -157.77857899258248\n",
      "Total T: 20200 Episode Num: 101 Episode T: 200 Reward: -121.033\n",
      "Total T: 20400 Episode Num: 102 Episode T: 200 Reward: -124.069\n",
      "Total T: 20600 Episode Num: 103 Episode T: 200 Reward: -121.480\n",
      "Total T: 20800 Episode Num: 104 Episode T: 200 Reward: -241.452\n",
      "Total T: 21000 Episode Num: 105 Episode T: 200 Reward: -123.437\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -169.402\n",
      "---------------------------------------\n",
      "recent Evaluation: -169.40165096899807\n",
      "Total T: 21200 Episode Num: 106 Episode T: 200 Reward: -121.216\n",
      "Total T: 21400 Episode Num: 107 Episode T: 200 Reward: -320.948\n",
      "Total T: 21600 Episode Num: 108 Episode T: 200 Reward: -124.036\n",
      "Total T: 21800 Episode Num: 109 Episode T: 200 Reward: -124.663\n",
      "Total T: 22000 Episode Num: 110 Episode T: 200 Reward: -243.603\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -129.191\n",
      "---------------------------------------\n",
      "recent Evaluation: -129.1909068686405\n",
      "Total T: 22200 Episode Num: 111 Episode T: 200 Reward: -115.076\n",
      "Total T: 22400 Episode Num: 112 Episode T: 200 Reward: -124.215\n",
      "Total T: 22600 Episode Num: 113 Episode T: 200 Reward: -125.836\n",
      "Total T: 22800 Episode Num: 114 Episode T: 200 Reward: -125.084\n",
      "Total T: 23000 Episode Num: 115 Episode T: 200 Reward: -123.143\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -165.016\n",
      "---------------------------------------\n",
      "recent Evaluation: -165.01563897185667\n",
      "Total T: 23200 Episode Num: 116 Episode T: 200 Reward: -235.420\n",
      "Total T: 23400 Episode Num: 117 Episode T: 200 Reward: -2.740\n",
      "Total T: 23600 Episode Num: 118 Episode T: 200 Reward: -124.447\n",
      "Total T: 23800 Episode Num: 119 Episode T: 200 Reward: -318.036\n",
      "Total T: 24000 Episode Num: 120 Episode T: 200 Reward: -121.050\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -131.664\n",
      "---------------------------------------\n",
      "recent Evaluation: -131.66359639056367\n",
      "Total T: 24200 Episode Num: 121 Episode T: 200 Reward: -239.740\n",
      "Total T: 24400 Episode Num: 122 Episode T: 200 Reward: -114.858\n",
      "Total T: 24600 Episode Num: 123 Episode T: 200 Reward: -119.747\n",
      "Total T: 24800 Episode Num: 124 Episode T: 200 Reward: -1.346\n",
      "Total T: 25000 Episode Num: 125 Episode T: 200 Reward: -119.889\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.957\n",
      "---------------------------------------\n",
      "recent Evaluation: -119.95711288354676\n",
      "Total T: 25200 Episode Num: 126 Episode T: 200 Reward: -118.434\n",
      "Total T: 25400 Episode Num: 127 Episode T: 200 Reward: -118.394\n",
      "Total T: 25600 Episode Num: 128 Episode T: 200 Reward: -333.324\n",
      "Total T: 25800 Episode Num: 129 Episode T: 200 Reward: -116.563\n",
      "Total T: 26000 Episode Num: 130 Episode T: 200 Reward: -117.303\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -131.002\n",
      "---------------------------------------\n",
      "recent Evaluation: -131.00218155767863\n",
      "Total T: 26200 Episode Num: 131 Episode T: 200 Reward: -1.183\n",
      "Total T: 26400 Episode Num: 132 Episode T: 200 Reward: -300.544\n",
      "Total T: 26600 Episode Num: 133 Episode T: 200 Reward: -1.372\n",
      "Total T: 26800 Episode Num: 134 Episode T: 200 Reward: -115.885\n",
      "Total T: 27000 Episode Num: 135 Episode T: 200 Reward: -3.106\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -158.272\n",
      "---------------------------------------\n",
      "recent Evaluation: -158.2718862996646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 27200 Episode Num: 136 Episode T: 200 Reward: -124.867\n",
      "Total T: 27400 Episode Num: 137 Episode T: 200 Reward: -116.600\n",
      "Total T: 27600 Episode Num: 138 Episode T: 200 Reward: -122.501\n",
      "Total T: 27800 Episode Num: 139 Episode T: 200 Reward: -238.190\n",
      "Total T: 28000 Episode Num: 140 Episode T: 200 Reward: -2.345\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -154.398\n",
      "---------------------------------------\n",
      "recent Evaluation: -154.39750302612705\n",
      "Total T: 28200 Episode Num: 141 Episode T: 200 Reward: -348.221\n",
      "Total T: 28400 Episode Num: 142 Episode T: 200 Reward: -116.590\n",
      "Total T: 28600 Episode Num: 143 Episode T: 200 Reward: -340.794\n",
      "Total T: 28800 Episode Num: 144 Episode T: 200 Reward: -116.426\n",
      "Total T: 29000 Episode Num: 145 Episode T: 200 Reward: -122.065\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.250\n",
      "---------------------------------------\n",
      "recent Evaluation: -132.2496221276701\n",
      "Total T: 29200 Episode Num: 146 Episode T: 200 Reward: -121.936\n",
      "Total T: 29400 Episode Num: 147 Episode T: 200 Reward: -0.598\n",
      "Total T: 29600 Episode Num: 148 Episode T: 200 Reward: -0.953\n",
      "Total T: 29800 Episode Num: 149 Episode T: 200 Reward: -235.791\n",
      "Total T: 30000 Episode Num: 150 Episode T: 200 Reward: -1.584\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -130.951\n",
      "---------------------------------------\n",
      "recent Evaluation: -130.95110335129775\n",
      "Total T: 30200 Episode Num: 151 Episode T: 200 Reward: -120.337\n",
      "Total T: 30400 Episode Num: 152 Episode T: 200 Reward: -118.339\n",
      "Total T: 30600 Episode Num: 153 Episode T: 200 Reward: -1.587\n",
      "Total T: 30800 Episode Num: 154 Episode T: 200 Reward: -335.617\n",
      "Total T: 31000 Episode Num: 155 Episode T: 200 Reward: -2.201\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -117.221\n",
      "---------------------------------------\n",
      "recent Evaluation: -117.22064145345173\n",
      "Total T: 31200 Episode Num: 156 Episode T: 200 Reward: -341.100\n",
      "Total T: 31400 Episode Num: 157 Episode T: 200 Reward: -121.905\n",
      "Total T: 31600 Episode Num: 158 Episode T: 200 Reward: -321.697\n",
      "Total T: 31800 Episode Num: 159 Episode T: 200 Reward: -269.430\n",
      "Total T: 32000 Episode Num: 160 Episode T: 200 Reward: -0.809\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -152.844\n",
      "---------------------------------------\n",
      "recent Evaluation: -152.84425061109107\n",
      "Total T: 32200 Episode Num: 161 Episode T: 200 Reward: -233.978\n",
      "Total T: 32400 Episode Num: 162 Episode T: 200 Reward: -2.956\n",
      "Total T: 32600 Episode Num: 163 Episode T: 200 Reward: -233.626\n",
      "Total T: 32800 Episode Num: 164 Episode T: 200 Reward: -115.001\n",
      "Total T: 33000 Episode Num: 165 Episode T: 200 Reward: -118.747\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -129.202\n",
      "---------------------------------------\n",
      "recent Evaluation: -129.20207961185733\n",
      "Total T: 33200 Episode Num: 166 Episode T: 200 Reward: -121.069\n",
      "Total T: 33400 Episode Num: 167 Episode T: 200 Reward: -226.097\n",
      "Total T: 33600 Episode Num: 168 Episode T: 200 Reward: -123.041\n",
      "Total T: 33800 Episode Num: 169 Episode T: 200 Reward: -239.845\n",
      "Total T: 34000 Episode Num: 170 Episode T: 200 Reward: -235.725\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -141.954\n",
      "---------------------------------------\n",
      "recent Evaluation: -141.95389597178678\n",
      "Total T: 34200 Episode Num: 171 Episode T: 200 Reward: -116.159\n",
      "Total T: 34400 Episode Num: 172 Episode T: 200 Reward: -118.480\n",
      "Total T: 34600 Episode Num: 173 Episode T: 200 Reward: -121.288\n",
      "Total T: 34800 Episode Num: 174 Episode T: 200 Reward: -126.512\n",
      "Total T: 35000 Episode Num: 175 Episode T: 200 Reward: -128.077\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -134.990\n",
      "---------------------------------------\n",
      "recent Evaluation: -134.99035363165072\n",
      "Total T: 35200 Episode Num: 176 Episode T: 200 Reward: -122.401\n",
      "Total T: 35400 Episode Num: 177 Episode T: 200 Reward: -119.649\n",
      "Total T: 35600 Episode Num: 178 Episode T: 200 Reward: -116.778\n",
      "Total T: 35800 Episode Num: 179 Episode T: 200 Reward: -227.048\n",
      "Total T: 36000 Episode Num: 180 Episode T: 200 Reward: -226.133\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -129.292\n",
      "---------------------------------------\n",
      "recent Evaluation: -129.29181319279763\n",
      "Total T: 36200 Episode Num: 181 Episode T: 200 Reward: -118.931\n",
      "Total T: 36400 Episode Num: 182 Episode T: 200 Reward: -123.128\n",
      "Total T: 36600 Episode Num: 183 Episode T: 200 Reward: -254.606\n",
      "Total T: 36800 Episode Num: 184 Episode T: 200 Reward: -117.093\n",
      "Total T: 37000 Episode Num: 185 Episode T: 200 Reward: -120.030\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -162.006\n",
      "---------------------------------------\n",
      "recent Evaluation: -162.00560539856775\n",
      "Total T: 37200 Episode Num: 186 Episode T: 200 Reward: -121.006\n",
      "Total T: 37400 Episode Num: 187 Episode T: 200 Reward: -123.914\n",
      "Total T: 37600 Episode Num: 188 Episode T: 200 Reward: -244.216\n",
      "Total T: 37800 Episode Num: 189 Episode T: 200 Reward: -308.979\n",
      "Total T: 38000 Episode Num: 190 Episode T: 200 Reward: -129.554\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -153.565\n",
      "---------------------------------------\n",
      "recent Evaluation: -153.56492539891246\n",
      "Total T: 38200 Episode Num: 191 Episode T: 200 Reward: -119.609\n",
      "Total T: 38400 Episode Num: 192 Episode T: 200 Reward: -124.968\n",
      "Total T: 38600 Episode Num: 193 Episode T: 200 Reward: -224.183\n",
      "Total T: 38800 Episode Num: 194 Episode T: 200 Reward: -227.725\n",
      "Total T: 39000 Episode Num: 195 Episode T: 200 Reward: -332.412\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -130.920\n",
      "---------------------------------------\n",
      "recent Evaluation: -130.91966474824204\n",
      "Total T: 39200 Episode Num: 196 Episode T: 200 Reward: -2.034\n",
      "Total T: 39400 Episode Num: 197 Episode T: 200 Reward: -2.394\n",
      "Total T: 39600 Episode Num: 198 Episode T: 200 Reward: -226.378\n",
      "Total T: 39800 Episode Num: 199 Episode T: 200 Reward: -120.793\n",
      "Total T: 40000 Episode Num: 200 Episode T: 200 Reward: -122.055\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.148\n",
      "---------------------------------------\n",
      "recent Evaluation: -132.1477309796498\n",
      "Total T: 40200 Episode Num: 201 Episode T: 200 Reward: -120.450\n",
      "Total T: 40400 Episode Num: 202 Episode T: 200 Reward: -239.482\n",
      "Total T: 40600 Episode Num: 203 Episode T: 200 Reward: -236.573\n",
      "Total T: 40800 Episode Num: 204 Episode T: 200 Reward: -119.614\n",
      "Total T: 41000 Episode Num: 205 Episode T: 200 Reward: -124.951\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -165.914\n",
      "---------------------------------------\n",
      "recent Evaluation: -165.9136590202553\n",
      "Total T: 41200 Episode Num: 206 Episode T: 200 Reward: -122.238\n",
      "Total T: 41400 Episode Num: 207 Episode T: 200 Reward: -120.821\n",
      "Total T: 41600 Episode Num: 208 Episode T: 200 Reward: -120.547\n",
      "Total T: 41800 Episode Num: 209 Episode T: 200 Reward: -224.553\n",
      "Total T: 42000 Episode Num: 210 Episode T: 200 Reward: -125.293\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -156.622\n",
      "---------------------------------------\n",
      "recent Evaluation: -156.622085374134\n",
      "Total T: 42200 Episode Num: 211 Episode T: 200 Reward: -229.266\n",
      "Total T: 42400 Episode Num: 212 Episode T: 200 Reward: -233.381\n",
      "Total T: 42600 Episode Num: 213 Episode T: 200 Reward: -2.646\n",
      "Total T: 42800 Episode Num: 214 Episode T: 200 Reward: -254.360\n",
      "Total T: 43000 Episode Num: 215 Episode T: 200 Reward: -232.361\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -166.101\n",
      "---------------------------------------\n",
      "recent Evaluation: -166.1012996009917\n",
      "Total T: 43200 Episode Num: 216 Episode T: 200 Reward: -116.031\n",
      "Total T: 43400 Episode Num: 217 Episode T: 200 Reward: -3.088\n",
      "Total T: 43600 Episode Num: 218 Episode T: 200 Reward: -126.580\n",
      "Total T: 43800 Episode Num: 219 Episode T: 200 Reward: -126.705\n",
      "Total T: 44000 Episode Num: 220 Episode T: 200 Reward: -3.697\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -158.865\n",
      "---------------------------------------\n",
      "recent Evaluation: -158.8653165462736\n",
      "Total T: 44200 Episode Num: 221 Episode T: 200 Reward: -119.201\n",
      "Total T: 44400 Episode Num: 222 Episode T: 200 Reward: -119.710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 44600 Episode Num: 223 Episode T: 200 Reward: -124.464\n",
      "Total T: 44800 Episode Num: 224 Episode T: 200 Reward: -121.163\n",
      "Total T: 45000 Episode Num: 225 Episode T: 200 Reward: -119.441\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -123.576\n",
      "---------------------------------------\n",
      "recent Evaluation: -123.57637149751876\n",
      "Total T: 45200 Episode Num: 226 Episode T: 200 Reward: -126.586\n",
      "Total T: 45400 Episode Num: 227 Episode T: 200 Reward: -130.468\n",
      "Total T: 45600 Episode Num: 228 Episode T: 200 Reward: -126.949\n",
      "Total T: 45800 Episode Num: 229 Episode T: 200 Reward: -126.292\n",
      "Total T: 46000 Episode Num: 230 Episode T: 200 Reward: -350.398\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -156.166\n",
      "---------------------------------------\n",
      "recent Evaluation: -156.16602651961415\n",
      "Total T: 46200 Episode Num: 231 Episode T: 200 Reward: -122.756\n",
      "Total T: 46400 Episode Num: 232 Episode T: 200 Reward: -118.177\n",
      "Total T: 46600 Episode Num: 233 Episode T: 200 Reward: -241.450\n",
      "Total T: 46800 Episode Num: 234 Episode T: 200 Reward: -126.975\n",
      "Total T: 47000 Episode Num: 235 Episode T: 200 Reward: -117.755\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -121.470\n",
      "---------------------------------------\n",
      "recent Evaluation: -121.469764170742\n",
      "Total T: 47200 Episode Num: 236 Episode T: 200 Reward: -345.902\n",
      "Total T: 47400 Episode Num: 237 Episode T: 200 Reward: -133.473\n",
      "Total T: 47600 Episode Num: 238 Episode T: 200 Reward: -4.637\n",
      "Total T: 47800 Episode Num: 239 Episode T: 200 Reward: -3.969\n",
      "Total T: 48000 Episode Num: 240 Episode T: 200 Reward: -122.579\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -147.524\n",
      "---------------------------------------\n",
      "recent Evaluation: -147.52410964365154\n",
      "Total T: 48200 Episode Num: 241 Episode T: 200 Reward: -120.398\n",
      "Total T: 48400 Episode Num: 242 Episode T: 200 Reward: -129.608\n",
      "Total T: 48600 Episode Num: 243 Episode T: 200 Reward: -225.945\n",
      "Total T: 48800 Episode Num: 244 Episode T: 200 Reward: -127.499\n",
      "Total T: 49000 Episode Num: 245 Episode T: 200 Reward: -230.439\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -174.461\n",
      "---------------------------------------\n",
      "recent Evaluation: -174.46107070661543\n",
      "Total T: 49200 Episode Num: 246 Episode T: 200 Reward: -4.821\n",
      "Total T: 49400 Episode Num: 247 Episode T: 200 Reward: -257.667\n",
      "Total T: 49600 Episode Num: 248 Episode T: 200 Reward: -233.055\n",
      "Total T: 49800 Episode Num: 249 Episode T: 200 Reward: -123.802\n",
      "Total T: 50000 Episode Num: 250 Episode T: 200 Reward: -233.716\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -145.329\n",
      "---------------------------------------\n",
      "recent Evaluation: -145.32868059816127\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1240.205\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1313.178\n",
      "---------------------------------------\n",
      "Total T: 200 Episode Num: 1 Episode T: 200 Reward: -1507.845\n",
      "Total T: 400 Episode Num: 2 Episode T: 200 Reward: -660.836\n",
      "Total T: 600 Episode Num: 3 Episode T: 200 Reward: -1566.669\n",
      "Total T: 800 Episode Num: 4 Episode T: 200 Reward: -1300.806\n",
      "Total T: 1000 Episode Num: 5 Episode T: 200 Reward: -1276.020\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1234.879\n",
      "---------------------------------------\n",
      "recent Evaluation: -1234.8793708658256\n",
      "Total T: 1200 Episode Num: 6 Episode T: 200 Reward: -1516.284\n",
      "Total T: 1400 Episode Num: 7 Episode T: 200 Reward: -1573.263\n",
      "Total T: 1600 Episode Num: 8 Episode T: 200 Reward: -1389.633\n",
      "Total T: 1800 Episode Num: 9 Episode T: 200 Reward: -1638.401\n",
      "Total T: 2000 Episode Num: 10 Episode T: 200 Reward: -1185.863\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1112.388\n",
      "---------------------------------------\n",
      "recent Evaluation: -1112.3876132868822\n",
      "Total T: 2200 Episode Num: 11 Episode T: 200 Reward: -1151.657\n",
      "Total T: 2400 Episode Num: 12 Episode T: 200 Reward: -1556.466\n",
      "Total T: 2600 Episode Num: 13 Episode T: 200 Reward: -857.942\n",
      "Total T: 2800 Episode Num: 14 Episode T: 200 Reward: -890.971\n",
      "Total T: 3000 Episode Num: 15 Episode T: 200 Reward: -1457.175\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1227.161\n",
      "---------------------------------------\n",
      "recent Evaluation: -1227.1610409695236\n",
      "Total T: 3200 Episode Num: 16 Episode T: 200 Reward: -1723.775\n",
      "Total T: 3400 Episode Num: 17 Episode T: 200 Reward: -1340.224\n",
      "Total T: 3600 Episode Num: 18 Episode T: 200 Reward: -1519.704\n",
      "Total T: 3800 Episode Num: 19 Episode T: 200 Reward: -1453.124\n",
      "Total T: 4000 Episode Num: 20 Episode T: 200 Reward: -1414.188\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1361.750\n",
      "---------------------------------------\n",
      "recent Evaluation: -1361.7501082730826\n",
      "Total T: 4200 Episode Num: 21 Episode T: 200 Reward: -987.484\n",
      "Total T: 4400 Episode Num: 22 Episode T: 200 Reward: -877.762\n",
      "Total T: 4600 Episode Num: 23 Episode T: 200 Reward: -1246.325\n",
      "Total T: 4800 Episode Num: 24 Episode T: 200 Reward: -1722.621\n",
      "Total T: 5000 Episode Num: 25 Episode T: 200 Reward: -1831.124\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1100.778\n",
      "---------------------------------------\n",
      "recent Evaluation: -1100.7777420634632\n",
      "Total T: 5200 Episode Num: 26 Episode T: 200 Reward: -1256.681\n",
      "Total T: 5400 Episode Num: 27 Episode T: 200 Reward: -1068.238\n",
      "Total T: 5600 Episode Num: 28 Episode T: 200 Reward: -1086.677\n",
      "Total T: 5800 Episode Num: 29 Episode T: 200 Reward: -1408.405\n",
      "Total T: 6000 Episode Num: 30 Episode T: 200 Reward: -1287.336\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1234.533\n",
      "---------------------------------------\n",
      "recent Evaluation: -1234.5326705634895\n",
      "Total T: 6200 Episode Num: 31 Episode T: 200 Reward: -1255.468\n",
      "Total T: 6400 Episode Num: 32 Episode T: 200 Reward: -864.982\n",
      "Total T: 6600 Episode Num: 33 Episode T: 200 Reward: -869.515\n",
      "Total T: 6800 Episode Num: 34 Episode T: 200 Reward: -1208.807\n",
      "Total T: 7000 Episode Num: 35 Episode T: 200 Reward: -916.662\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1217.671\n",
      "---------------------------------------\n",
      "recent Evaluation: -1217.670761033469\n",
      "Total T: 7200 Episode Num: 36 Episode T: 200 Reward: -1458.223\n",
      "Total T: 7400 Episode Num: 37 Episode T: 200 Reward: -1163.748\n",
      "Total T: 7600 Episode Num: 38 Episode T: 200 Reward: -768.889\n",
      "Total T: 7800 Episode Num: 39 Episode T: 200 Reward: -1321.962\n",
      "Total T: 8000 Episode Num: 40 Episode T: 200 Reward: -1208.531\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1296.983\n",
      "---------------------------------------\n",
      "recent Evaluation: -1296.9833920685999\n",
      "Total T: 8200 Episode Num: 41 Episode T: 200 Reward: -1381.578\n",
      "Total T: 8400 Episode Num: 42 Episode T: 200 Reward: -1768.424\n",
      "Total T: 8600 Episode Num: 43 Episode T: 200 Reward: -866.010\n",
      "Total T: 8800 Episode Num: 44 Episode T: 200 Reward: -1272.721\n",
      "Total T: 9000 Episode Num: 45 Episode T: 200 Reward: -1328.679\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1095.070\n",
      "---------------------------------------\n",
      "recent Evaluation: -1095.0696035334784\n",
      "Total T: 9200 Episode Num: 46 Episode T: 200 Reward: -1748.542\n",
      "Total T: 9400 Episode Num: 47 Episode T: 200 Reward: -966.947\n",
      "Total T: 9600 Episode Num: 48 Episode T: 200 Reward: -1259.597\n",
      "Total T: 9800 Episode Num: 49 Episode T: 200 Reward: -1375.405\n",
      "Total T: 10000 Episode Num: 50 Episode T: 200 Reward: -1079.198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invase: Syn5 300 . Y\n",
      "invase: Syn1 2500 . Y\n",
      "input shape: 3\n",
      "Epoch:      0, d_loss (Acc)): 0.132, v_loss (Acc): 0.170, g_loss: +5.1377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1226.941\n",
      "---------------------------------------\n",
      "recent Evaluation: -1226.9412058098767\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "(5000, 3) (5000, 101) (5000, 3) (5000, 3) (5000, 101) (5000, 3) (5000, 101)\n",
      "start training......\n",
      "now at training epoch number 0 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 70.2%  std: 45.7%\n",
      "FDR mean: 98.6%  std: 0.9%\n",
      "Epoch:      0, d_loss (Acc)): 0.013, v_loss (Acc): 0.021, g_loss: +3.5915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.99687822 10.30139719]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "now at training epoch number 100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 90.9%  std: 28.8%\n",
      "FDR mean: 98.1%  std: 0.6%\n",
      "Epoch:      0, d_loss (Acc)): 0.010, v_loss (Acc): 0.007, g_loss: -1.5068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.71202304 8.49535338]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 96.8%  std: 17.5%\n",
      "FDR mean: 98.0%  std: 0.4%\n",
      "Epoch:      0, d_loss (Acc)): 0.010, v_loss (Acc): 0.007, g_loss: -1.6202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.6331217  8.37579312]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 98.6%  std: 11.9%\n",
      "FDR mean: 97.9%  std: 0.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.009, v_loss (Acc): 0.006, g_loss: -1.1825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.59350119 8.24436681]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.2%  std: 8.7%\n",
      "FDR mean: 97.7%  std: 0.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.008, v_loss (Acc): 0.006, g_loss: -0.6398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.56842859 8.15318434]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 500 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.7%  std: 5.6%\n",
      "FDR mean: 97.5%  std: 0.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.007, v_loss (Acc): 0.006, g_loss: -0.4944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.5226007 7.9595547]\n",
      " [0.        0.       ]\n",
      " [0.        0.       ]]\n",
      "now at training epoch number 600 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.9%  std: 2.4%\n",
      "FDR mean: 97.2%  std: 0.4%\n",
      "Epoch:      0, d_loss (Acc)): 0.006, v_loss (Acc): 0.005, g_loss: -0.3787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.51404663 7.75168392]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 700 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 1.4%\n",
      "FDR mean: 96.6%  std: 0.6%\n",
      "Epoch:      0, d_loss (Acc)): 0.006, v_loss (Acc): 0.005, g_loss: -0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.4460078 7.4689423]\n",
      " [0.        0.       ]\n",
      " [0.        0.       ]]\n",
      "now at training epoch number 800 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 95.6%  std: 1.1%\n",
      "Epoch:      0, d_loss (Acc)): 0.005, v_loss (Acc): 0.005, g_loss: -0.1332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.44326295 7.07832042]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 900 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 93.6%  std: 2.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.005, v_loss (Acc): 0.005, g_loss: -0.1660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.39670035 6.61974646]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1000 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 89.3%  std: 4.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.005, v_loss (Acc): 0.004, g_loss: -0.0739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.34909947 5.97921566]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 78.5%  std: 12.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.005, v_loss (Acc): 0.004, g_loss: +0.1117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.3342375  5.44900722]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 52.2%  std: 27.4%\n",
      "Epoch:      0, d_loss (Acc)): 0.004, v_loss (Acc): 0.004, g_loss: +0.2818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.29843355 5.20388209]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 14.1%  std: 25.9%\n",
      "Epoch:      0, d_loss (Acc)): 0.004, v_loss (Acc): 0.004, g_loss: +0.4970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.2674987 4.8709583]\n",
      " [0.        0.       ]\n",
      " [0.        0.       ]]\n",
      "now at training epoch number 1400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 2.9%  std: 13.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.002, v_loss (Acc): 0.003, g_loss: +0.6961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.23769623 4.434782  ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1500 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.7%  std: 6.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.002, v_loss (Acc): 0.003, g_loss: +0.7958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.21624818 3.95826025]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1600 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.2%  std: 3.2%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +0.7372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.15764364 3.38389195]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1700 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.1%  std: 1.8%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +0.6883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.15999889 2.81215072]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1800 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 1.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +0.6078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.11024826 2.28281649]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1900 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.002, g_loss: +0.4728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.07635841 1.96044197]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2000 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.4505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.07584813 1.70700626]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.002, g_loss: +0.3799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.02629744 1.54806896]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.2952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.96032253 1.4135955 ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.2584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.94087507 1.35606331]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.91297351 1.20222243]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "PyTorch Version: elapsed time for Syn1: 11 feature, 10000 sample: [248.1456] sec.\n",
      "Total T: 10200 Episode Num: 51 Episode T: 200 Reward: -1470.285\n",
      "Total T: 10400 Episode Num: 52 Episode T: 200 Reward: -1892.845\n",
      "Total T: 10600 Episode Num: 53 Episode T: 200 Reward: -1897.560\n",
      "Total T: 10800 Episode Num: 54 Episode T: 200 Reward: -1522.005\n",
      "Total T: 11000 Episode Num: 55 Episode T: 200 Reward: -1481.015\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1443.279\n",
      "---------------------------------------\n",
      "recent Evaluation: -1443.278507582909\n",
      "Total T: 11200 Episode Num: 56 Episode T: 200 Reward: -1526.883\n",
      "Total T: 11400 Episode Num: 57 Episode T: 200 Reward: -1511.341\n",
      "Total T: 11600 Episode Num: 58 Episode T: 200 Reward: -1480.475\n",
      "Total T: 11800 Episode Num: 59 Episode T: 200 Reward: -1518.332\n",
      "Total T: 12000 Episode Num: 60 Episode T: 200 Reward: -1503.810\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1380.057\n",
      "---------------------------------------\n",
      "recent Evaluation: -1380.0570434947253\n",
      "Total T: 12200 Episode Num: 61 Episode T: 200 Reward: -1187.396\n",
      "Total T: 12400 Episode Num: 62 Episode T: 200 Reward: -1195.947\n",
      "Total T: 12600 Episode Num: 63 Episode T: 200 Reward: -1226.787\n",
      "Total T: 12800 Episode Num: 64 Episode T: 200 Reward: -1218.334\n",
      "Total T: 13000 Episode Num: 65 Episode T: 200 Reward: -1513.684\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -927.675\n",
      "---------------------------------------\n",
      "recent Evaluation: -927.6753460496714\n",
      "Total T: 13200 Episode Num: 66 Episode T: 200 Reward: -1525.853\n",
      "Total T: 13400 Episode Num: 67 Episode T: 200 Reward: -142.805\n",
      "Total T: 13600 Episode Num: 68 Episode T: 200 Reward: -716.841\n",
      "Total T: 13800 Episode Num: 69 Episode T: 200 Reward: -135.941\n",
      "Total T: 14000 Episode Num: 70 Episode T: 200 Reward: -413.602\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -157.224\n",
      "---------------------------------------\n",
      "recent Evaluation: -157.22351958175972\n",
      "Total T: 14200 Episode Num: 71 Episode T: 200 Reward: -129.706\n",
      "Total T: 14400 Episode Num: 72 Episode T: 200 Reward: -1513.563\n",
      "Total T: 14600 Episode Num: 73 Episode T: 200 Reward: -129.780\n",
      "Total T: 14800 Episode Num: 74 Episode T: 200 Reward: -520.452\n",
      "Total T: 15000 Episode Num: 75 Episode T: 200 Reward: -132.202\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -146.997\n",
      "---------------------------------------\n",
      "recent Evaluation: -146.99662493260445\n",
      "Total T: 15200 Episode Num: 76 Episode T: 200 Reward: -3.252\n",
      "Total T: 15400 Episode Num: 77 Episode T: 200 Reward: -131.710\n",
      "Total T: 15600 Episode Num: 78 Episode T: 200 Reward: -131.743\n",
      "Total T: 15800 Episode Num: 79 Episode T: 200 Reward: -262.873\n",
      "Total T: 16000 Episode Num: 80 Episode T: 200 Reward: -4.166\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -174.467\n",
      "---------------------------------------\n",
      "recent Evaluation: -174.467231164765\n",
      "Total T: 16200 Episode Num: 81 Episode T: 200 Reward: -130.416\n",
      "Total T: 16400 Episode Num: 82 Episode T: 200 Reward: -130.977\n",
      "Total T: 16600 Episode Num: 83 Episode T: 200 Reward: -128.357\n",
      "Total T: 16800 Episode Num: 84 Episode T: 200 Reward: -127.086\n",
      "Total T: 17000 Episode Num: 85 Episode T: 200 Reward: -6.172\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -121.613\n",
      "---------------------------------------\n",
      "recent Evaluation: -121.61345527505208\n",
      "Total T: 17200 Episode Num: 86 Episode T: 200 Reward: -132.098\n",
      "Total T: 17400 Episode Num: 87 Episode T: 200 Reward: -121.658\n",
      "Total T: 17600 Episode Num: 88 Episode T: 200 Reward: -120.235\n",
      "Total T: 17800 Episode Num: 89 Episode T: 200 Reward: -378.123\n",
      "Total T: 18000 Episode Num: 90 Episode T: 200 Reward: -123.193\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -146.289\n",
      "---------------------------------------\n",
      "recent Evaluation: -146.28938040223179\n",
      "Total T: 18200 Episode Num: 91 Episode T: 200 Reward: -125.483\n",
      "Total T: 18400 Episode Num: 92 Episode T: 200 Reward: -240.864\n",
      "Total T: 18600 Episode Num: 93 Episode T: 200 Reward: -123.549\n",
      "Total T: 18800 Episode Num: 94 Episode T: 200 Reward: -115.401\n",
      "Total T: 19000 Episode Num: 95 Episode T: 200 Reward: -126.397\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -124.305\n",
      "---------------------------------------\n",
      "recent Evaluation: -124.30496091860388\n",
      "Total T: 19200 Episode Num: 96 Episode T: 200 Reward: -128.944\n",
      "Total T: 19400 Episode Num: 97 Episode T: 200 Reward: -309.966\n",
      "Total T: 19600 Episode Num: 98 Episode T: 200 Reward: -1.074\n",
      "Total T: 19800 Episode Num: 99 Episode T: 200 Reward: -124.156\n",
      "Total T: 20000 Episode Num: 100 Episode T: 200 Reward: -351.929\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -202.278\n",
      "---------------------------------------\n",
      "recent Evaluation: -202.27825063247542\n",
      "Total T: 20200 Episode Num: 101 Episode T: 200 Reward: -119.429\n",
      "Total T: 20400 Episode Num: 102 Episode T: 200 Reward: -237.389\n",
      "Total T: 20600 Episode Num: 103 Episode T: 200 Reward: -119.046\n",
      "Total T: 20800 Episode Num: 104 Episode T: 200 Reward: -118.224\n",
      "Total T: 21000 Episode Num: 105 Episode T: 200 Reward: -121.049\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -175.636\n",
      "---------------------------------------\n",
      "recent Evaluation: -175.63555423985275\n",
      "Total T: 21200 Episode Num: 106 Episode T: 200 Reward: -123.276\n",
      "Total T: 21400 Episode Num: 107 Episode T: 200 Reward: -118.359\n",
      "Total T: 21600 Episode Num: 108 Episode T: 200 Reward: -352.611\n",
      "Total T: 21800 Episode Num: 109 Episode T: 200 Reward: -115.133\n",
      "Total T: 22000 Episode Num: 110 Episode T: 200 Reward: -126.832\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -154.351\n",
      "---------------------------------------\n",
      "recent Evaluation: -154.35133004618825\n",
      "Total T: 22200 Episode Num: 111 Episode T: 200 Reward: -244.536\n",
      "Total T: 22400 Episode Num: 112 Episode T: 200 Reward: -3.364\n",
      "Total T: 22600 Episode Num: 113 Episode T: 200 Reward: -123.249\n",
      "Total T: 22800 Episode Num: 114 Episode T: 200 Reward: -117.704\n",
      "Total T: 23000 Episode Num: 115 Episode T: 200 Reward: -117.110\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -136.319\n",
      "---------------------------------------\n",
      "recent Evaluation: -136.3185651688638\n",
      "Total T: 23200 Episode Num: 116 Episode T: 200 Reward: -2.197\n",
      "Total T: 23400 Episode Num: 117 Episode T: 200 Reward: -117.263\n",
      "Total T: 23600 Episode Num: 118 Episode T: 200 Reward: -120.306\n",
      "Total T: 23800 Episode Num: 119 Episode T: 200 Reward: -120.896\n",
      "Total T: 24000 Episode Num: 120 Episode T: 200 Reward: -121.603\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -150.139\n",
      "---------------------------------------\n",
      "recent Evaluation: -150.13947265068438\n",
      "Total T: 24200 Episode Num: 121 Episode T: 200 Reward: -117.990\n",
      "Total T: 24400 Episode Num: 122 Episode T: 200 Reward: -127.780\n",
      "Total T: 24600 Episode Num: 123 Episode T: 200 Reward: -124.247\n",
      "Total T: 24800 Episode Num: 124 Episode T: 200 Reward: -329.139\n",
      "Total T: 25000 Episode Num: 125 Episode T: 200 Reward: -246.908\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -115.076\n",
      "---------------------------------------\n",
      "recent Evaluation: -115.07558276546906\n",
      "Total T: 25200 Episode Num: 126 Episode T: 200 Reward: -118.256\n",
      "Total T: 25400 Episode Num: 127 Episode T: 200 Reward: -117.522\n",
      "Total T: 25600 Episode Num: 128 Episode T: 200 Reward: -238.558\n",
      "Total T: 25800 Episode Num: 129 Episode T: 200 Reward: -2.521\n",
      "Total T: 26000 Episode Num: 130 Episode T: 200 Reward: -118.980\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -144.892\n",
      "---------------------------------------\n",
      "recent Evaluation: -144.89161926728934\n",
      "Total T: 26200 Episode Num: 131 Episode T: 200 Reward: -117.606\n",
      "Total T: 26400 Episode Num: 132 Episode T: 200 Reward: -228.294\n",
      "Total T: 26600 Episode Num: 133 Episode T: 200 Reward: -118.146\n",
      "Total T: 26800 Episode Num: 134 Episode T: 200 Reward: -119.891\n",
      "Total T: 27000 Episode Num: 135 Episode T: 200 Reward: -117.503\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -136.231\n",
      "---------------------------------------\n",
      "recent Evaluation: -136.23124846867444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 27200 Episode Num: 136 Episode T: 200 Reward: -249.611\n",
      "Total T: 27400 Episode Num: 137 Episode T: 200 Reward: -121.816\n",
      "Total T: 27600 Episode Num: 138 Episode T: 200 Reward: -119.581\n",
      "Total T: 27800 Episode Num: 139 Episode T: 200 Reward: -120.169\n",
      "Total T: 28000 Episode Num: 140 Episode T: 200 Reward: -115.718\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -154.858\n",
      "---------------------------------------\n",
      "recent Evaluation: -154.85766384552232\n",
      "Total T: 28200 Episode Num: 141 Episode T: 200 Reward: -115.530\n",
      "Total T: 28400 Episode Num: 142 Episode T: 200 Reward: -118.832\n",
      "Total T: 28600 Episode Num: 143 Episode T: 200 Reward: -119.349\n",
      "Total T: 28800 Episode Num: 144 Episode T: 200 Reward: -118.797\n",
      "Total T: 29000 Episode Num: 145 Episode T: 200 Reward: -0.445\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -129.446\n",
      "---------------------------------------\n",
      "recent Evaluation: -129.4463580501267\n",
      "Total T: 29200 Episode Num: 146 Episode T: 200 Reward: -233.865\n",
      "Total T: 29400 Episode Num: 147 Episode T: 200 Reward: -123.279\n",
      "Total T: 29600 Episode Num: 148 Episode T: 200 Reward: -0.678\n",
      "Total T: 29800 Episode Num: 149 Episode T: 200 Reward: -270.115\n",
      "Total T: 30000 Episode Num: 150 Episode T: 200 Reward: -223.144\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -171.544\n",
      "---------------------------------------\n",
      "recent Evaluation: -171.54422434551583\n",
      "Total T: 30200 Episode Num: 151 Episode T: 200 Reward: -235.074\n",
      "Total T: 30400 Episode Num: 152 Episode T: 200 Reward: -114.186\n",
      "Total T: 30600 Episode Num: 153 Episode T: 200 Reward: -310.359\n",
      "Total T: 30800 Episode Num: 154 Episode T: 200 Reward: -122.171\n",
      "Total T: 31000 Episode Num: 155 Episode T: 200 Reward: -2.653\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -167.607\n",
      "---------------------------------------\n",
      "recent Evaluation: -167.60672653545933\n",
      "Total T: 31200 Episode Num: 156 Episode T: 200 Reward: -229.861\n",
      "Total T: 31400 Episode Num: 157 Episode T: 200 Reward: -121.109\n",
      "Total T: 31600 Episode Num: 158 Episode T: 200 Reward: -114.285\n",
      "Total T: 31800 Episode Num: 159 Episode T: 200 Reward: -122.008\n",
      "Total T: 32000 Episode Num: 160 Episode T: 200 Reward: -119.705\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -191.418\n",
      "---------------------------------------\n",
      "recent Evaluation: -191.41769536045598\n",
      "Total T: 32200 Episode Num: 161 Episode T: 200 Reward: -117.778\n",
      "Total T: 32400 Episode Num: 162 Episode T: 200 Reward: -115.577\n",
      "Total T: 32600 Episode Num: 163 Episode T: 200 Reward: -231.952\n",
      "Total T: 32800 Episode Num: 164 Episode T: 200 Reward: -122.763\n",
      "Total T: 33000 Episode Num: 165 Episode T: 200 Reward: -0.164\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -155.910\n",
      "---------------------------------------\n",
      "recent Evaluation: -155.9097353728772\n",
      "Total T: 33200 Episode Num: 166 Episode T: 200 Reward: -118.521\n",
      "Total T: 33400 Episode Num: 167 Episode T: 200 Reward: -230.241\n",
      "Total T: 33600 Episode Num: 168 Episode T: 200 Reward: -231.349\n",
      "Total T: 33800 Episode Num: 169 Episode T: 200 Reward: -230.105\n",
      "Total T: 34000 Episode Num: 170 Episode T: 200 Reward: -0.447\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -150.802\n",
      "---------------------------------------\n",
      "recent Evaluation: -150.80247257802938\n",
      "Total T: 34200 Episode Num: 171 Episode T: 200 Reward: -329.581\n",
      "Total T: 34400 Episode Num: 172 Episode T: 200 Reward: -117.316\n",
      "Total T: 34600 Episode Num: 173 Episode T: 200 Reward: -224.655\n",
      "Total T: 34800 Episode Num: 174 Episode T: 200 Reward: -121.308\n",
      "Total T: 35000 Episode Num: 175 Episode T: 200 Reward: -119.456\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -171.716\n",
      "---------------------------------------\n",
      "recent Evaluation: -171.7157924834942\n",
      "Total T: 35200 Episode Num: 176 Episode T: 200 Reward: -0.318\n",
      "Total T: 35400 Episode Num: 177 Episode T: 200 Reward: -1.420\n",
      "Total T: 35600 Episode Num: 178 Episode T: 200 Reward: -115.183\n",
      "Total T: 35800 Episode Num: 179 Episode T: 200 Reward: -237.446\n",
      "Total T: 36000 Episode Num: 180 Episode T: 200 Reward: -244.813\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -129.099\n",
      "---------------------------------------\n",
      "recent Evaluation: -129.0988610693047\n",
      "Total T: 36200 Episode Num: 181 Episode T: 200 Reward: -236.051\n",
      "Total T: 36400 Episode Num: 182 Episode T: 200 Reward: -120.004\n",
      "Total T: 36600 Episode Num: 183 Episode T: 200 Reward: -234.131\n",
      "Total T: 36800 Episode Num: 184 Episode T: 200 Reward: -222.905\n",
      "Total T: 37000 Episode Num: 185 Episode T: 200 Reward: -122.936\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -170.554\n",
      "---------------------------------------\n",
      "recent Evaluation: -170.55364139038898\n",
      "Total T: 37200 Episode Num: 186 Episode T: 200 Reward: -119.552\n",
      "Total T: 37400 Episode Num: 187 Episode T: 200 Reward: -3.261\n",
      "Total T: 37600 Episode Num: 188 Episode T: 200 Reward: -118.501\n",
      "Total T: 37800 Episode Num: 189 Episode T: 200 Reward: -118.223\n",
      "Total T: 38000 Episode Num: 190 Episode T: 200 Reward: -282.966\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.641\n",
      "---------------------------------------\n",
      "recent Evaluation: -132.64120982795492\n",
      "Total T: 38200 Episode Num: 191 Episode T: 200 Reward: -2.610\n",
      "Total T: 38400 Episode Num: 192 Episode T: 200 Reward: -123.415\n",
      "Total T: 38600 Episode Num: 193 Episode T: 200 Reward: -121.537\n",
      "Total T: 38800 Episode Num: 194 Episode T: 200 Reward: -126.195\n",
      "Total T: 39000 Episode Num: 195 Episode T: 200 Reward: -120.148\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -166.387\n",
      "---------------------------------------\n",
      "recent Evaluation: -166.3873915725974\n",
      "Total T: 39200 Episode Num: 196 Episode T: 200 Reward: -2.327\n",
      "Total T: 39400 Episode Num: 197 Episode T: 200 Reward: -239.991\n",
      "Total T: 39600 Episode Num: 198 Episode T: 200 Reward: -121.556\n",
      "Total T: 39800 Episode Num: 199 Episode T: 200 Reward: -119.387\n",
      "Total T: 40000 Episode Num: 200 Episode T: 200 Reward: -123.275\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -146.258\n",
      "---------------------------------------\n",
      "recent Evaluation: -146.2579969699539\n",
      "Total T: 40200 Episode Num: 201 Episode T: 200 Reward: -231.127\n",
      "Total T: 40400 Episode Num: 202 Episode T: 200 Reward: -235.803\n",
      "Total T: 40600 Episode Num: 203 Episode T: 200 Reward: -4.369\n",
      "Total T: 40800 Episode Num: 204 Episode T: 200 Reward: -242.142\n",
      "Total T: 41000 Episode Num: 205 Episode T: 200 Reward: -2.240\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -115.748\n",
      "---------------------------------------\n",
      "recent Evaluation: -115.74842380934004\n",
      "Total T: 41200 Episode Num: 206 Episode T: 200 Reward: -347.238\n",
      "Total T: 41400 Episode Num: 207 Episode T: 200 Reward: -1.801\n",
      "Total T: 41600 Episode Num: 208 Episode T: 200 Reward: -118.756\n",
      "Total T: 41800 Episode Num: 209 Episode T: 200 Reward: -128.280\n",
      "Total T: 42000 Episode Num: 210 Episode T: 200 Reward: -232.069\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -130.147\n",
      "---------------------------------------\n",
      "recent Evaluation: -130.14704167723116\n",
      "Total T: 42200 Episode Num: 211 Episode T: 200 Reward: -115.609\n",
      "Total T: 42400 Episode Num: 212 Episode T: 200 Reward: -237.117\n",
      "Total T: 42600 Episode Num: 213 Episode T: 200 Reward: -272.264\n",
      "Total T: 42800 Episode Num: 214 Episode T: 200 Reward: -115.774\n",
      "Total T: 43000 Episode Num: 215 Episode T: 200 Reward: -117.237\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.738\n",
      "---------------------------------------\n",
      "recent Evaluation: -132.7377253488419\n",
      "Total T: 43200 Episode Num: 216 Episode T: 200 Reward: -125.579\n",
      "Total T: 43400 Episode Num: 217 Episode T: 200 Reward: -230.090\n",
      "Total T: 43600 Episode Num: 218 Episode T: 200 Reward: -237.517\n",
      "Total T: 43800 Episode Num: 219 Episode T: 200 Reward: -351.839\n",
      "Total T: 44000 Episode Num: 220 Episode T: 200 Reward: -114.710\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.041\n",
      "---------------------------------------\n",
      "recent Evaluation: -142.04114232623417\n",
      "Total T: 44200 Episode Num: 221 Episode T: 200 Reward: -234.885\n",
      "Total T: 44400 Episode Num: 222 Episode T: 200 Reward: -2.232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 44600 Episode Num: 223 Episode T: 200 Reward: -117.204\n",
      "Total T: 44800 Episode Num: 224 Episode T: 200 Reward: -120.919\n",
      "Total T: 45000 Episode Num: 225 Episode T: 200 Reward: -230.348\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -172.171\n",
      "---------------------------------------\n",
      "recent Evaluation: -172.1707715654045\n",
      "Total T: 45200 Episode Num: 226 Episode T: 200 Reward: -117.491\n",
      "Total T: 45400 Episode Num: 227 Episode T: 200 Reward: -116.959\n",
      "Total T: 45600 Episode Num: 228 Episode T: 200 Reward: -225.147\n",
      "Total T: 45800 Episode Num: 229 Episode T: 200 Reward: -116.358\n",
      "Total T: 46000 Episode Num: 230 Episode T: 200 Reward: -237.696\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -121.607\n",
      "---------------------------------------\n",
      "recent Evaluation: -121.60677270286976\n",
      "Total T: 46200 Episode Num: 231 Episode T: 200 Reward: -125.037\n",
      "Total T: 46400 Episode Num: 232 Episode T: 200 Reward: -118.795\n",
      "Total T: 46600 Episode Num: 233 Episode T: 200 Reward: -116.048\n",
      "Total T: 46800 Episode Num: 234 Episode T: 200 Reward: -274.587\n",
      "Total T: 47000 Episode Num: 235 Episode T: 200 Reward: -120.008\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -121.976\n",
      "---------------------------------------\n",
      "recent Evaluation: -121.97584081838741\n",
      "Total T: 47200 Episode Num: 236 Episode T: 200 Reward: -115.488\n",
      "Total T: 47400 Episode Num: 237 Episode T: 200 Reward: -2.093\n",
      "Total T: 47600 Episode Num: 238 Episode T: 200 Reward: -119.475\n",
      "Total T: 47800 Episode Num: 239 Episode T: 200 Reward: -340.504\n",
      "Total T: 48000 Episode Num: 240 Episode T: 200 Reward: -126.566\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -133.082\n",
      "---------------------------------------\n",
      "recent Evaluation: -133.08205379959188\n",
      "Total T: 48200 Episode Num: 241 Episode T: 200 Reward: -226.275\n",
      "Total T: 48400 Episode Num: 242 Episode T: 200 Reward: -123.950\n",
      "Total T: 48600 Episode Num: 243 Episode T: 200 Reward: -122.181\n",
      "Total T: 48800 Episode Num: 244 Episode T: 200 Reward: -1.195\n",
      "Total T: 49000 Episode Num: 245 Episode T: 200 Reward: -233.049\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -107.662\n",
      "---------------------------------------\n",
      "recent Evaluation: -107.66152953710366\n",
      "Total T: 49200 Episode Num: 246 Episode T: 200 Reward: -3.879\n",
      "Total T: 49400 Episode Num: 247 Episode T: 200 Reward: -242.205\n",
      "Total T: 49600 Episode Num: 248 Episode T: 200 Reward: -357.294\n",
      "Total T: 49800 Episode Num: 249 Episode T: 200 Reward: -124.570\n",
      "Total T: 50000 Episode Num: 250 Episode T: 200 Reward: -249.400\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -155.954\n",
      "---------------------------------------\n",
      "recent Evaluation: -155.9539260468193\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1273.488\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1204.323\n",
      "---------------------------------------\n",
      "Total T: 200 Episode Num: 1 Episode T: 200 Reward: -1342.030\n",
      "Total T: 400 Episode Num: 2 Episode T: 200 Reward: -1609.485\n",
      "Total T: 600 Episode Num: 3 Episode T: 200 Reward: -768.139\n",
      "Total T: 800 Episode Num: 4 Episode T: 200 Reward: -1171.702\n",
      "Total T: 1000 Episode Num: 5 Episode T: 200 Reward: -1758.607\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1212.509\n",
      "---------------------------------------\n",
      "recent Evaluation: -1212.5087363959087\n",
      "Total T: 1200 Episode Num: 6 Episode T: 200 Reward: -890.367\n",
      "Total T: 1400 Episode Num: 7 Episode T: 200 Reward: -1294.272\n",
      "Total T: 1600 Episode Num: 8 Episode T: 200 Reward: -1275.810\n",
      "Total T: 1800 Episode Num: 9 Episode T: 200 Reward: -1314.205\n",
      "Total T: 2000 Episode Num: 10 Episode T: 200 Reward: -1278.070\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1226.999\n",
      "---------------------------------------\n",
      "recent Evaluation: -1226.99944832339\n",
      "Total T: 2200 Episode Num: 11 Episode T: 200 Reward: -974.878\n",
      "Total T: 2400 Episode Num: 12 Episode T: 200 Reward: -918.133\n",
      "Total T: 2600 Episode Num: 13 Episode T: 200 Reward: -1782.449\n",
      "Total T: 2800 Episode Num: 14 Episode T: 200 Reward: -1450.326\n",
      "Total T: 3000 Episode Num: 15 Episode T: 200 Reward: -1479.615\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1185.652\n",
      "---------------------------------------\n",
      "recent Evaluation: -1185.6523677696755\n",
      "Total T: 3200 Episode Num: 16 Episode T: 200 Reward: -1280.000\n",
      "Total T: 3400 Episode Num: 17 Episode T: 200 Reward: -1346.250\n",
      "Total T: 3600 Episode Num: 18 Episode T: 200 Reward: -1630.135\n",
      "Total T: 3800 Episode Num: 19 Episode T: 200 Reward: -1209.595\n",
      "Total T: 4000 Episode Num: 20 Episode T: 200 Reward: -1085.230\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1213.707\n",
      "---------------------------------------\n",
      "recent Evaluation: -1213.7065048313232\n",
      "Total T: 4200 Episode Num: 21 Episode T: 200 Reward: -1530.239\n",
      "Total T: 4400 Episode Num: 22 Episode T: 200 Reward: -959.114\n",
      "Total T: 4600 Episode Num: 23 Episode T: 200 Reward: -980.374\n",
      "Total T: 4800 Episode Num: 24 Episode T: 200 Reward: -1560.476\n",
      "Total T: 5000 Episode Num: 25 Episode T: 200 Reward: -1764.766\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1250.351\n",
      "---------------------------------------\n",
      "recent Evaluation: -1250.3511025389548\n",
      "Total T: 5200 Episode Num: 26 Episode T: 200 Reward: -1204.526\n",
      "Total T: 5400 Episode Num: 27 Episode T: 200 Reward: -1352.492\n",
      "Total T: 5600 Episode Num: 28 Episode T: 200 Reward: -1554.888\n",
      "Total T: 5800 Episode Num: 29 Episode T: 200 Reward: -866.776\n",
      "Total T: 6000 Episode Num: 30 Episode T: 200 Reward: -899.244\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1304.899\n",
      "---------------------------------------\n",
      "recent Evaluation: -1304.8990845639455\n",
      "Total T: 6200 Episode Num: 31 Episode T: 200 Reward: -1340.826\n",
      "Total T: 6400 Episode Num: 32 Episode T: 200 Reward: -1781.449\n",
      "Total T: 6600 Episode Num: 33 Episode T: 200 Reward: -1189.107\n",
      "Total T: 6800 Episode Num: 34 Episode T: 200 Reward: -1568.030\n",
      "Total T: 7000 Episode Num: 35 Episode T: 200 Reward: -1741.089\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1130.236\n",
      "---------------------------------------\n",
      "recent Evaluation: -1130.236309178109\n",
      "Total T: 7200 Episode Num: 36 Episode T: 200 Reward: -1322.871\n",
      "Total T: 7400 Episode Num: 37 Episode T: 200 Reward: -1134.815\n",
      "Total T: 7600 Episode Num: 38 Episode T: 200 Reward: -1414.725\n",
      "Total T: 7800 Episode Num: 39 Episode T: 200 Reward: -978.848\n",
      "Total T: 8000 Episode Num: 40 Episode T: 200 Reward: -1608.513\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1270.352\n",
      "---------------------------------------\n",
      "recent Evaluation: -1270.3523171349282\n",
      "Total T: 8200 Episode Num: 41 Episode T: 200 Reward: -1656.849\n",
      "Total T: 8400 Episode Num: 42 Episode T: 200 Reward: -1283.503\n",
      "Total T: 8600 Episode Num: 43 Episode T: 200 Reward: -1171.064\n",
      "Total T: 8800 Episode Num: 44 Episode T: 200 Reward: -864.878\n",
      "Total T: 9000 Episode Num: 45 Episode T: 200 Reward: -1495.720\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1246.695\n",
      "---------------------------------------\n",
      "recent Evaluation: -1246.695364702347\n",
      "Total T: 9200 Episode Num: 46 Episode T: 200 Reward: -955.690\n",
      "Total T: 9400 Episode Num: 47 Episode T: 200 Reward: -857.003\n",
      "Total T: 9600 Episode Num: 48 Episode T: 200 Reward: -998.640\n",
      "Total T: 9800 Episode Num: 49 Episode T: 200 Reward: -965.474\n",
      "Total T: 10000 Episode Num: 50 Episode T: 200 Reward: -1071.068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invase: Syn5 300 . Y\n",
      "invase: Syn1 2500 . Y\n",
      "input shape: 3\n",
      "Epoch:      0, d_loss (Acc)): 0.158, v_loss (Acc): 0.181, g_loss: +3.0518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1237.780\n",
      "---------------------------------------\n",
      "recent Evaluation: -1237.7798074394357\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "(5000, 3) (5000, 101) (5000, 3) (5000, 3) (5000, 101) (5000, 3) (5000, 101)\n",
      "start training......\n",
      "now at training epoch number 0 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 51.3%  std: 50.0%\n",
      "FDR mean: 99.0%  std: 1.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.015, v_loss (Acc): 0.015, g_loss: +0.2780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.26240881 11.98523907]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "now at training epoch number 100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 76.7%  std: 42.3%\n",
      "FDR mean: 98.4%  std: 0.9%\n",
      "Epoch:      0, d_loss (Acc)): 0.011, v_loss (Acc): 0.008, g_loss: -1.5955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.59715498 8.19791247]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 91.0%  std: 28.6%\n",
      "FDR mean: 98.1%  std: 0.6%\n",
      "Epoch:      0, d_loss (Acc)): 0.010, v_loss (Acc): 0.007, g_loss: -1.3815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.50549645 8.08755093]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 96.7%  std: 17.8%\n",
      "FDR mean: 97.9%  std: 0.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.010, v_loss (Acc): 0.008, g_loss: -1.0804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.46187195 7.98010032]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 98.9%  std: 10.2%\n",
      "FDR mean: 97.7%  std: 0.4%\n",
      "Epoch:      0, d_loss (Acc)): 0.009, v_loss (Acc): 0.007, g_loss: -0.9378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.41921702 7.84953108]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 500 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.9%  std: 3.5%\n",
      "FDR mean: 97.4%  std: 0.4%\n",
      "Epoch:      0, d_loss (Acc)): 0.008, v_loss (Acc): 0.006, g_loss: -0.6084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.33723732 7.68108324]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 600 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 2.0%\n",
      "FDR mean: 96.9%  std: 0.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.007, v_loss (Acc): 0.006, g_loss: -0.5340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.31876618 7.42793303]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 700 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 96.1%  std: 0.8%\n",
      "Epoch:      0, d_loss (Acc)): 0.006, v_loss (Acc): 0.005, g_loss: -0.2160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.27891052 7.11414071]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 800 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 94.5%  std: 1.6%\n",
      "Epoch:      0, d_loss (Acc)): 0.006, v_loss (Acc): 0.005, g_loss: -0.3986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.23757673 6.62078794]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 900 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 91.1%  std: 3.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.006, v_loss (Acc): 0.005, g_loss: -0.2207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.18894848 6.07201983]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1000 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 81.9%  std: 10.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.005, v_loss (Acc): 0.005, g_loss: -0.1063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.11222471 5.44489681]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 57.4%  std: 27.1%\n",
      "Epoch:      0, d_loss (Acc)): 0.005, v_loss (Acc): 0.004, g_loss: +0.1525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.06081704 5.23654483]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 22.9%  std: 29.7%\n",
      "Epoch:      0, d_loss (Acc)): 0.004, v_loss (Acc): 0.004, g_loss: +0.2946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.95057451 5.04333175]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 6.6%  std: 18.6%\n",
      "Epoch:      0, d_loss (Acc)): 0.003, v_loss (Acc): 0.004, g_loss: +0.6071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.8867288  4.56934286]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 1.3%  std: 8.6%\n",
      "Epoch:      0, d_loss (Acc)): 0.002, v_loss (Acc): 0.003, g_loss: +0.8151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.76947835 4.04787588]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1500 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.3%  std: 4.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.002, v_loss (Acc): 0.003, g_loss: +0.8308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.64540102 3.44605529]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1600 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 1.6%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +0.7683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.50718853 2.88346358]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1700 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +0.7543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.37889902 2.35084223]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1800 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +0.6238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.21721895 1.96858354]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1900 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.5160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.02680516 1.73365917]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2000 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.4009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.83074412 1.53810844]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.3548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.64185543 1.40439588]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.2857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.45404723 1.27470015]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.2498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.31973664 1.15663587]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.17298815 1.07375525]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "PyTorch Version: elapsed time for Syn1: 11 feature, 10000 sample: [248.2686] sec.\n",
      "Total T: 10200 Episode Num: 51 Episode T: 200 Reward: -1595.317\n",
      "Total T: 10400 Episode Num: 52 Episode T: 200 Reward: -1343.515\n",
      "Total T: 10600 Episode Num: 53 Episode T: 200 Reward: -1687.217\n",
      "Total T: 10800 Episode Num: 54 Episode T: 200 Reward: -1561.207\n",
      "Total T: 11000 Episode Num: 55 Episode T: 200 Reward: -1593.242\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1535.392\n",
      "---------------------------------------\n",
      "recent Evaluation: -1535.3921261376854\n",
      "Total T: 11200 Episode Num: 56 Episode T: 200 Reward: -1521.368\n",
      "Total T: 11400 Episode Num: 57 Episode T: 200 Reward: -1539.733\n",
      "Total T: 11600 Episode Num: 58 Episode T: 200 Reward: -1522.140\n",
      "Total T: 11800 Episode Num: 59 Episode T: 200 Reward: -1526.152\n",
      "Total T: 12000 Episode Num: 60 Episode T: 200 Reward: -1459.885\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1277.227\n",
      "---------------------------------------\n",
      "recent Evaluation: -1277.2270703518145\n",
      "Total T: 12200 Episode Num: 61 Episode T: 200 Reward: -1272.326\n",
      "Total T: 12400 Episode Num: 62 Episode T: 200 Reward: -1512.784\n",
      "Total T: 12600 Episode Num: 63 Episode T: 200 Reward: -1192.316\n",
      "Total T: 12800 Episode Num: 64 Episode T: 200 Reward: -1095.197\n",
      "Total T: 13000 Episode Num: 65 Episode T: 200 Reward: -1.076\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -927.045\n",
      "---------------------------------------\n",
      "recent Evaluation: -927.0445164423343\n",
      "Total T: 13200 Episode Num: 66 Episode T: 200 Reward: -937.024\n",
      "Total T: 13400 Episode Num: 67 Episode T: 200 Reward: -800.740\n",
      "Total T: 13600 Episode Num: 68 Episode T: 200 Reward: -0.158\n",
      "Total T: 13800 Episode Num: 69 Episode T: 200 Reward: -135.514\n",
      "Total T: 14000 Episode Num: 70 Episode T: 200 Reward: -403.394\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -692.991\n",
      "---------------------------------------\n",
      "recent Evaluation: -692.9913599340249\n",
      "Total T: 14200 Episode Num: 71 Episode T: 200 Reward: -136.322\n",
      "Total T: 14400 Episode Num: 72 Episode T: 200 Reward: -137.533\n",
      "Total T: 14600 Episode Num: 73 Episode T: 200 Reward: -280.894\n",
      "Total T: 14800 Episode Num: 74 Episode T: 200 Reward: -130.119\n",
      "Total T: 15000 Episode Num: 75 Episode T: 200 Reward: -125.501\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -169.725\n",
      "---------------------------------------\n",
      "recent Evaluation: -169.72504567620075\n",
      "Total T: 15200 Episode Num: 76 Episode T: 200 Reward: -126.020\n",
      "Total T: 15400 Episode Num: 77 Episode T: 200 Reward: -133.903\n",
      "Total T: 15600 Episode Num: 78 Episode T: 200 Reward: -351.365\n",
      "Total T: 15800 Episode Num: 79 Episode T: 200 Reward: -124.345\n",
      "Total T: 16000 Episode Num: 80 Episode T: 200 Reward: -365.742\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -178.966\n",
      "---------------------------------------\n",
      "recent Evaluation: -178.96596556851972\n",
      "Total T: 16200 Episode Num: 81 Episode T: 200 Reward: -122.748\n",
      "Total T: 16400 Episode Num: 82 Episode T: 200 Reward: -125.837\n",
      "Total T: 16600 Episode Num: 83 Episode T: 200 Reward: -331.201\n",
      "Total T: 16800 Episode Num: 84 Episode T: 200 Reward: -122.063\n",
      "Total T: 17000 Episode Num: 85 Episode T: 200 Reward: -122.299\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -192.765\n",
      "---------------------------------------\n",
      "recent Evaluation: -192.76502406943143\n",
      "Total T: 17200 Episode Num: 86 Episode T: 200 Reward: -125.812\n",
      "Total T: 17400 Episode Num: 87 Episode T: 200 Reward: -0.354\n",
      "Total T: 17600 Episode Num: 88 Episode T: 200 Reward: -1.032\n",
      "Total T: 17800 Episode Num: 89 Episode T: 200 Reward: -129.385\n",
      "Total T: 18000 Episode Num: 90 Episode T: 200 Reward: -123.061\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -149.000\n",
      "---------------------------------------\n",
      "recent Evaluation: -149.0002197136651\n",
      "Total T: 18200 Episode Num: 91 Episode T: 200 Reward: -124.735\n",
      "Total T: 18400 Episode Num: 92 Episode T: 200 Reward: -124.170\n",
      "Total T: 18600 Episode Num: 93 Episode T: 200 Reward: -344.189\n",
      "Total T: 18800 Episode Num: 94 Episode T: 200 Reward: -122.515\n",
      "Total T: 19000 Episode Num: 95 Episode T: 200 Reward: -128.096\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.661\n",
      "---------------------------------------\n",
      "recent Evaluation: -122.66104877734556\n",
      "Total T: 19200 Episode Num: 96 Episode T: 200 Reward: -126.357\n",
      "Total T: 19400 Episode Num: 97 Episode T: 200 Reward: -357.968\n",
      "Total T: 19600 Episode Num: 98 Episode T: 200 Reward: -119.955\n",
      "Total T: 19800 Episode Num: 99 Episode T: 200 Reward: -236.596\n",
      "Total T: 20000 Episode Num: 100 Episode T: 200 Reward: -242.744\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.201\n",
      "---------------------------------------\n",
      "recent Evaluation: -122.20087196439758\n",
      "Total T: 20200 Episode Num: 101 Episode T: 200 Reward: -241.196\n",
      "Total T: 20400 Episode Num: 102 Episode T: 200 Reward: -1.074\n",
      "Total T: 20600 Episode Num: 103 Episode T: 200 Reward: -336.578\n",
      "Total T: 20800 Episode Num: 104 Episode T: 200 Reward: -242.186\n",
      "Total T: 21000 Episode Num: 105 Episode T: 200 Reward: -123.589\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.983\n",
      "---------------------------------------\n",
      "recent Evaluation: -132.9834553553741\n",
      "Total T: 21200 Episode Num: 106 Episode T: 200 Reward: -235.777\n",
      "Total T: 21400 Episode Num: 107 Episode T: 200 Reward: -2.116\n",
      "Total T: 21600 Episode Num: 108 Episode T: 200 Reward: -119.523\n",
      "Total T: 21800 Episode Num: 109 Episode T: 200 Reward: -123.474\n",
      "Total T: 22000 Episode Num: 110 Episode T: 200 Reward: -126.183\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -133.950\n",
      "---------------------------------------\n",
      "recent Evaluation: -133.9496426203438\n",
      "Total T: 22200 Episode Num: 111 Episode T: 200 Reward: -336.067\n",
      "Total T: 22400 Episode Num: 112 Episode T: 200 Reward: -119.798\n",
      "Total T: 22600 Episode Num: 113 Episode T: 200 Reward: -124.578\n",
      "Total T: 22800 Episode Num: 114 Episode T: 200 Reward: -114.037\n",
      "Total T: 23000 Episode Num: 115 Episode T: 200 Reward: -1.855\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -164.521\n",
      "---------------------------------------\n",
      "recent Evaluation: -164.52060730445803\n",
      "Total T: 23200 Episode Num: 116 Episode T: 200 Reward: -0.872\n",
      "Total T: 23400 Episode Num: 117 Episode T: 200 Reward: -121.464\n",
      "Total T: 23600 Episode Num: 118 Episode T: 200 Reward: -246.210\n",
      "Total T: 23800 Episode Num: 119 Episode T: 200 Reward: -124.301\n",
      "Total T: 24000 Episode Num: 120 Episode T: 200 Reward: -115.742\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -129.931\n",
      "---------------------------------------\n",
      "recent Evaluation: -129.93108744830053\n",
      "Total T: 24200 Episode Num: 121 Episode T: 200 Reward: -279.472\n",
      "Total T: 24400 Episode Num: 122 Episode T: 200 Reward: -234.080\n",
      "Total T: 24600 Episode Num: 123 Episode T: 200 Reward: -1.109\n",
      "Total T: 24800 Episode Num: 124 Episode T: 200 Reward: -242.899\n",
      "Total T: 25000 Episode Num: 125 Episode T: 200 Reward: -1.422\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -167.334\n",
      "---------------------------------------\n",
      "recent Evaluation: -167.33384889681798\n",
      "Total T: 25200 Episode Num: 126 Episode T: 200 Reward: -227.088\n",
      "Total T: 25400 Episode Num: 127 Episode T: 200 Reward: -230.695\n",
      "Total T: 25600 Episode Num: 128 Episode T: 200 Reward: -230.473\n",
      "Total T: 25800 Episode Num: 129 Episode T: 200 Reward: -269.427\n",
      "Total T: 26000 Episode Num: 130 Episode T: 200 Reward: -233.927\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -173.362\n",
      "---------------------------------------\n",
      "recent Evaluation: -173.36202072988226\n",
      "Total T: 26200 Episode Num: 131 Episode T: 200 Reward: -248.765\n",
      "Total T: 26400 Episode Num: 132 Episode T: 200 Reward: -227.347\n",
      "Total T: 26600 Episode Num: 133 Episode T: 200 Reward: -120.347\n",
      "Total T: 26800 Episode Num: 134 Episode T: 200 Reward: -236.319\n",
      "Total T: 27000 Episode Num: 135 Episode T: 200 Reward: -346.497\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -153.725\n",
      "---------------------------------------\n",
      "recent Evaluation: -153.725202126722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 27200 Episode Num: 136 Episode T: 200 Reward: -1.010\n",
      "Total T: 27400 Episode Num: 137 Episode T: 200 Reward: -116.120\n",
      "Total T: 27600 Episode Num: 138 Episode T: 200 Reward: -116.650\n",
      "Total T: 27800 Episode Num: 139 Episode T: 200 Reward: -116.642\n",
      "Total T: 28000 Episode Num: 140 Episode T: 200 Reward: -118.741\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -130.120\n",
      "---------------------------------------\n",
      "recent Evaluation: -130.12029046493393\n",
      "Total T: 28200 Episode Num: 141 Episode T: 200 Reward: -232.904\n",
      "Total T: 28400 Episode Num: 142 Episode T: 200 Reward: -331.477\n",
      "Total T: 28600 Episode Num: 143 Episode T: 200 Reward: -114.322\n",
      "Total T: 28800 Episode Num: 144 Episode T: 200 Reward: -224.744\n",
      "Total T: 29000 Episode Num: 145 Episode T: 200 Reward: -119.191\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -149.846\n",
      "---------------------------------------\n",
      "recent Evaluation: -149.8463336644727\n",
      "Total T: 29200 Episode Num: 146 Episode T: 200 Reward: -120.815\n",
      "Total T: 29400 Episode Num: 147 Episode T: 200 Reward: -123.767\n",
      "Total T: 29600 Episode Num: 148 Episode T: 200 Reward: -229.405\n",
      "Total T: 29800 Episode Num: 149 Episode T: 200 Reward: -118.111\n",
      "Total T: 30000 Episode Num: 150 Episode T: 200 Reward: -119.895\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -59.698\n",
      "---------------------------------------\n",
      "recent Evaluation: -59.698282298332764\n",
      "Total T: 30200 Episode Num: 151 Episode T: 200 Reward: -3.895\n",
      "Total T: 30400 Episode Num: 152 Episode T: 200 Reward: -237.836\n",
      "Total T: 30600 Episode Num: 153 Episode T: 200 Reward: -116.728\n",
      "Total T: 30800 Episode Num: 154 Episode T: 200 Reward: -234.239\n",
      "Total T: 31000 Episode Num: 155 Episode T: 200 Reward: -336.696\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -129.518\n",
      "---------------------------------------\n",
      "recent Evaluation: -129.5182367362928\n",
      "Total T: 31200 Episode Num: 156 Episode T: 200 Reward: -225.167\n",
      "Total T: 31400 Episode Num: 157 Episode T: 200 Reward: -242.798\n",
      "Total T: 31600 Episode Num: 158 Episode T: 200 Reward: -118.798\n",
      "Total T: 31800 Episode Num: 159 Episode T: 200 Reward: -124.283\n",
      "Total T: 32000 Episode Num: 160 Episode T: 200 Reward: -114.610\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -137.590\n",
      "---------------------------------------\n",
      "recent Evaluation: -137.5902662839183\n",
      "Total T: 32200 Episode Num: 161 Episode T: 200 Reward: -119.505\n",
      "Total T: 32400 Episode Num: 162 Episode T: 200 Reward: -125.249\n",
      "Total T: 32600 Episode Num: 163 Episode T: 200 Reward: -227.311\n",
      "Total T: 32800 Episode Num: 164 Episode T: 200 Reward: -227.733\n",
      "Total T: 33000 Episode Num: 165 Episode T: 200 Reward: -118.285\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -171.310\n",
      "---------------------------------------\n",
      "recent Evaluation: -171.30971830973323\n",
      "Total T: 33200 Episode Num: 166 Episode T: 200 Reward: -118.872\n",
      "Total T: 33400 Episode Num: 167 Episode T: 200 Reward: -121.765\n",
      "Total T: 33600 Episode Num: 168 Episode T: 200 Reward: -246.117\n",
      "Total T: 33800 Episode Num: 169 Episode T: 200 Reward: -123.154\n",
      "Total T: 34000 Episode Num: 170 Episode T: 200 Reward: -119.627\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -180.915\n",
      "---------------------------------------\n",
      "recent Evaluation: -180.91528261022066\n",
      "Total T: 34200 Episode Num: 171 Episode T: 200 Reward: -120.068\n",
      "Total T: 34400 Episode Num: 172 Episode T: 200 Reward: -122.665\n",
      "Total T: 34600 Episode Num: 173 Episode T: 200 Reward: -121.617\n",
      "Total T: 34800 Episode Num: 174 Episode T: 200 Reward: -121.084\n",
      "Total T: 35000 Episode Num: 175 Episode T: 200 Reward: -344.523\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.654\n",
      "---------------------------------------\n",
      "recent Evaluation: -142.65356849381473\n",
      "Total T: 35200 Episode Num: 176 Episode T: 200 Reward: -118.485\n",
      "Total T: 35400 Episode Num: 177 Episode T: 200 Reward: -118.403\n",
      "Total T: 35600 Episode Num: 178 Episode T: 200 Reward: -4.659\n",
      "Total T: 35800 Episode Num: 179 Episode T: 200 Reward: -319.088\n",
      "Total T: 36000 Episode Num: 180 Episode T: 200 Reward: -4.103\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -158.269\n",
      "---------------------------------------\n",
      "recent Evaluation: -158.26885213347234\n",
      "Total T: 36200 Episode Num: 181 Episode T: 200 Reward: -122.215\n",
      "Total T: 36400 Episode Num: 182 Episode T: 200 Reward: -124.668\n",
      "Total T: 36600 Episode Num: 183 Episode T: 200 Reward: -244.606\n",
      "Total T: 36800 Episode Num: 184 Episode T: 200 Reward: -236.079\n",
      "Total T: 37000 Episode Num: 185 Episode T: 200 Reward: -117.773\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -164.302\n",
      "---------------------------------------\n",
      "recent Evaluation: -164.3022373146429\n",
      "Total T: 37200 Episode Num: 186 Episode T: 200 Reward: -122.794\n",
      "Total T: 37400 Episode Num: 187 Episode T: 200 Reward: -233.093\n",
      "Total T: 37600 Episode Num: 188 Episode T: 200 Reward: -123.965\n",
      "Total T: 37800 Episode Num: 189 Episode T: 200 Reward: -126.452\n",
      "Total T: 38000 Episode Num: 190 Episode T: 200 Reward: -130.366\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -125.336\n",
      "---------------------------------------\n",
      "recent Evaluation: -125.33625906383296\n",
      "Total T: 38200 Episode Num: 191 Episode T: 200 Reward: -130.763\n",
      "Total T: 38400 Episode Num: 192 Episode T: 200 Reward: -246.554\n",
      "Total T: 38600 Episode Num: 193 Episode T: 200 Reward: -9.645\n",
      "Total T: 38800 Episode Num: 194 Episode T: 200 Reward: -7.795\n",
      "Total T: 39000 Episode Num: 195 Episode T: 200 Reward: -8.784\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -184.457\n",
      "---------------------------------------\n",
      "recent Evaluation: -184.45747276153756\n",
      "Total T: 39200 Episode Num: 196 Episode T: 200 Reward: -233.966\n",
      "Total T: 39400 Episode Num: 197 Episode T: 200 Reward: -8.253\n",
      "Total T: 39600 Episode Num: 198 Episode T: 200 Reward: -344.420\n",
      "Total T: 39800 Episode Num: 199 Episode T: 200 Reward: -342.638\n",
      "Total T: 40000 Episode Num: 200 Episode T: 200 Reward: -120.700\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -121.490\n",
      "---------------------------------------\n",
      "recent Evaluation: -121.4900239159698\n",
      "Total T: 40200 Episode Num: 201 Episode T: 200 Reward: -232.549\n",
      "Total T: 40400 Episode Num: 202 Episode T: 200 Reward: -229.962\n",
      "Total T: 40600 Episode Num: 203 Episode T: 200 Reward: -117.598\n",
      "Total T: 40800 Episode Num: 204 Episode T: 200 Reward: -120.370\n",
      "Total T: 41000 Episode Num: 205 Episode T: 200 Reward: -117.610\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -89.774\n",
      "---------------------------------------\n",
      "recent Evaluation: -89.7738929792794\n",
      "Total T: 41200 Episode Num: 206 Episode T: 200 Reward: -226.314\n",
      "Total T: 41400 Episode Num: 207 Episode T: 200 Reward: -123.030\n",
      "Total T: 41600 Episode Num: 208 Episode T: 200 Reward: -121.825\n",
      "Total T: 41800 Episode Num: 209 Episode T: 200 Reward: -239.757\n",
      "Total T: 42000 Episode Num: 210 Episode T: 200 Reward: -231.709\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -143.370\n",
      "---------------------------------------\n",
      "recent Evaluation: -143.36996338585993\n",
      "Total T: 42200 Episode Num: 211 Episode T: 200 Reward: -276.959\n",
      "Total T: 42400 Episode Num: 212 Episode T: 200 Reward: -5.491\n",
      "Total T: 42600 Episode Num: 213 Episode T: 200 Reward: -244.661\n",
      "Total T: 42800 Episode Num: 214 Episode T: 200 Reward: -246.567\n",
      "Total T: 43000 Episode Num: 215 Episode T: 200 Reward: -5.090\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -102.561\n",
      "---------------------------------------\n",
      "recent Evaluation: -102.56066106491434\n",
      "Total T: 43200 Episode Num: 216 Episode T: 200 Reward: -6.584\n",
      "Total T: 43400 Episode Num: 217 Episode T: 200 Reward: -126.120\n",
      "Total T: 43600 Episode Num: 218 Episode T: 200 Reward: -122.383\n",
      "Total T: 43800 Episode Num: 219 Episode T: 200 Reward: -242.943\n",
      "Total T: 44000 Episode Num: 220 Episode T: 200 Reward: -241.353\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -110.943\n",
      "---------------------------------------\n",
      "recent Evaluation: -110.94304397751102\n",
      "Total T: 44200 Episode Num: 221 Episode T: 200 Reward: -234.129\n",
      "Total T: 44400 Episode Num: 222 Episode T: 200 Reward: -6.599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 44600 Episode Num: 223 Episode T: 200 Reward: -233.678\n",
      "Total T: 44800 Episode Num: 224 Episode T: 200 Reward: -124.772\n",
      "Total T: 45000 Episode Num: 225 Episode T: 200 Reward: -121.889\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -125.989\n",
      "---------------------------------------\n",
      "recent Evaluation: -125.98882117674846\n",
      "Total T: 45200 Episode Num: 226 Episode T: 200 Reward: -131.719\n",
      "Total T: 45400 Episode Num: 227 Episode T: 200 Reward: -229.990\n",
      "Total T: 45600 Episode Num: 228 Episode T: 200 Reward: -121.398\n",
      "Total T: 45800 Episode Num: 229 Episode T: 200 Reward: -121.778\n",
      "Total T: 46000 Episode Num: 230 Episode T: 200 Reward: -130.089\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -146.797\n",
      "---------------------------------------\n",
      "recent Evaluation: -146.79741256472988\n",
      "Total T: 46200 Episode Num: 231 Episode T: 200 Reward: -233.959\n",
      "Total T: 46400 Episode Num: 232 Episode T: 200 Reward: -237.035\n",
      "Total T: 46600 Episode Num: 233 Episode T: 200 Reward: -362.145\n",
      "Total T: 46800 Episode Num: 234 Episode T: 200 Reward: -128.965\n",
      "Total T: 47000 Episode Num: 235 Episode T: 200 Reward: -225.840\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -147.494\n",
      "---------------------------------------\n",
      "recent Evaluation: -147.49448706857046\n",
      "Total T: 47200 Episode Num: 236 Episode T: 200 Reward: -125.197\n",
      "Total T: 47400 Episode Num: 237 Episode T: 200 Reward: -129.931\n",
      "Total T: 47600 Episode Num: 238 Episode T: 200 Reward: -126.435\n",
      "Total T: 47800 Episode Num: 239 Episode T: 200 Reward: -118.889\n",
      "Total T: 48000 Episode Num: 240 Episode T: 200 Reward: -239.893\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -147.178\n",
      "---------------------------------------\n",
      "recent Evaluation: -147.1784324764015\n",
      "Total T: 48200 Episode Num: 241 Episode T: 200 Reward: -130.542\n",
      "Total T: 48400 Episode Num: 242 Episode T: 200 Reward: -6.924\n",
      "Total T: 48600 Episode Num: 243 Episode T: 200 Reward: -127.394\n",
      "Total T: 48800 Episode Num: 244 Episode T: 200 Reward: -245.054\n",
      "Total T: 49000 Episode Num: 245 Episode T: 200 Reward: -229.473\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -87.538\n",
      "---------------------------------------\n",
      "recent Evaluation: -87.53788282255209\n",
      "Total T: 49200 Episode Num: 246 Episode T: 200 Reward: -238.001\n",
      "Total T: 49400 Episode Num: 247 Episode T: 200 Reward: -244.775\n",
      "Total T: 49600 Episode Num: 248 Episode T: 200 Reward: -130.403\n",
      "Total T: 49800 Episode Num: 249 Episode T: 200 Reward: -329.261\n",
      "Total T: 50000 Episode Num: 250 Episode T: 200 Reward: -118.114\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -167.224\n",
      "---------------------------------------\n",
      "recent Evaluation: -167.22442249662228\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1236.370\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1304.389\n",
      "---------------------------------------\n",
      "Total T: 200 Episode Num: 1 Episode T: 200 Reward: -1203.981\n",
      "Total T: 400 Episode Num: 2 Episode T: 200 Reward: -1560.437\n",
      "Total T: 600 Episode Num: 3 Episode T: 200 Reward: -802.893\n",
      "Total T: 800 Episode Num: 4 Episode T: 200 Reward: -1215.181\n",
      "Total T: 1000 Episode Num: 5 Episode T: 200 Reward: -1594.808\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1288.226\n",
      "---------------------------------------\n",
      "recent Evaluation: -1288.2263799781927\n",
      "Total T: 1200 Episode Num: 6 Episode T: 200 Reward: -986.917\n",
      "Total T: 1400 Episode Num: 7 Episode T: 200 Reward: -1179.450\n",
      "Total T: 1600 Episode Num: 8 Episode T: 200 Reward: -1164.048\n",
      "Total T: 1800 Episode Num: 9 Episode T: 200 Reward: -998.762\n",
      "Total T: 2000 Episode Num: 10 Episode T: 200 Reward: -1224.114\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1249.738\n",
      "---------------------------------------\n",
      "recent Evaluation: -1249.7375738200603\n",
      "Total T: 2200 Episode Num: 11 Episode T: 200 Reward: -984.533\n",
      "Total T: 2400 Episode Num: 12 Episode T: 200 Reward: -747.956\n",
      "Total T: 2600 Episode Num: 13 Episode T: 200 Reward: -1632.782\n",
      "Total T: 2800 Episode Num: 14 Episode T: 200 Reward: -1175.410\n",
      "Total T: 3000 Episode Num: 15 Episode T: 200 Reward: -1565.379\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1243.566\n",
      "---------------------------------------\n",
      "recent Evaluation: -1243.565993834642\n",
      "Total T: 3200 Episode Num: 16 Episode T: 200 Reward: -1278.942\n",
      "Total T: 3400 Episode Num: 17 Episode T: 200 Reward: -1812.577\n",
      "Total T: 3600 Episode Num: 18 Episode T: 200 Reward: -1167.363\n",
      "Total T: 3800 Episode Num: 19 Episode T: 200 Reward: -904.080\n",
      "Total T: 4000 Episode Num: 20 Episode T: 200 Reward: -1153.332\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1258.335\n",
      "---------------------------------------\n",
      "recent Evaluation: -1258.3353777260481\n",
      "Total T: 4200 Episode Num: 21 Episode T: 200 Reward: -962.068\n",
      "Total T: 4400 Episode Num: 22 Episode T: 200 Reward: -1330.628\n",
      "Total T: 4600 Episode Num: 23 Episode T: 200 Reward: -1640.914\n",
      "Total T: 4800 Episode Num: 24 Episode T: 200 Reward: -990.113\n",
      "Total T: 5000 Episode Num: 25 Episode T: 200 Reward: -1359.875\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1248.690\n",
      "---------------------------------------\n",
      "recent Evaluation: -1248.6902924383255\n",
      "Total T: 5200 Episode Num: 26 Episode T: 200 Reward: -1094.052\n",
      "Total T: 5400 Episode Num: 27 Episode T: 200 Reward: -1162.261\n",
      "Total T: 5600 Episode Num: 28 Episode T: 200 Reward: -981.999\n",
      "Total T: 5800 Episode Num: 29 Episode T: 200 Reward: -1621.238\n",
      "Total T: 6000 Episode Num: 30 Episode T: 200 Reward: -1173.359\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1188.403\n",
      "---------------------------------------\n",
      "recent Evaluation: -1188.4032386166596\n",
      "Total T: 6200 Episode Num: 31 Episode T: 200 Reward: -1733.101\n",
      "Total T: 6400 Episode Num: 32 Episode T: 200 Reward: -1392.160\n",
      "Total T: 6600 Episode Num: 33 Episode T: 200 Reward: -1165.843\n",
      "Total T: 6800 Episode Num: 34 Episode T: 200 Reward: -1366.373\n",
      "Total T: 7000 Episode Num: 35 Episode T: 200 Reward: -1300.328\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1248.186\n",
      "---------------------------------------\n",
      "recent Evaluation: -1248.1855836056009\n",
      "Total T: 7200 Episode Num: 36 Episode T: 200 Reward: -822.381\n",
      "Total T: 7400 Episode Num: 37 Episode T: 200 Reward: -1048.623\n",
      "Total T: 7600 Episode Num: 38 Episode T: 200 Reward: -1148.023\n",
      "Total T: 7800 Episode Num: 39 Episode T: 200 Reward: -1563.898\n",
      "Total T: 8000 Episode Num: 40 Episode T: 200 Reward: -1051.798\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1208.201\n",
      "---------------------------------------\n",
      "recent Evaluation: -1208.2007678306854\n",
      "Total T: 8200 Episode Num: 41 Episode T: 200 Reward: -1287.976\n",
      "Total T: 8400 Episode Num: 42 Episode T: 200 Reward: -1259.623\n",
      "Total T: 8600 Episode Num: 43 Episode T: 200 Reward: -1702.493\n",
      "Total T: 8800 Episode Num: 44 Episode T: 200 Reward: -1116.203\n",
      "Total T: 9000 Episode Num: 45 Episode T: 200 Reward: -757.714\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1227.703\n",
      "---------------------------------------\n",
      "recent Evaluation: -1227.702944772484\n",
      "Total T: 9200 Episode Num: 46 Episode T: 200 Reward: -1011.120\n",
      "Total T: 9400 Episode Num: 47 Episode T: 200 Reward: -1255.336\n",
      "Total T: 9600 Episode Num: 48 Episode T: 200 Reward: -1489.044\n",
      "Total T: 9800 Episode Num: 49 Episode T: 200 Reward: -1738.442\n",
      "Total T: 10000 Episode Num: 50 Episode T: 200 Reward: -1256.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invase: Syn5 300 . Y\n",
      "invase: Syn1 2500 . Y\n",
      "input shape: 3\n",
      "Epoch:      0, d_loss (Acc)): 0.166, v_loss (Acc): 0.192, g_loss: +3.7827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1243.598\n",
      "---------------------------------------\n",
      "recent Evaluation: -1243.5984594970098\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "(5000, 3) (5000, 101) (5000, 3) (5000, 3) (5000, 101) (5000, 3) (5000, 101)\n",
      "start training......\n",
      "now at training epoch number 0 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 57.2%  std: 49.5%\n",
      "FDR mean: 98.8%  std: 1.1%\n",
      "Epoch:      0, d_loss (Acc)): 0.017, v_loss (Acc): 0.018, g_loss: +0.7563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.06122374 11.27666563]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "now at training epoch number 100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 82.8%  std: 37.7%\n",
      "FDR mean: 98.2%  std: 0.9%\n",
      "Epoch:      0, d_loss (Acc)): 0.013, v_loss (Acc): 0.010, g_loss: -1.4452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.50011809 8.24863635]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 93.9%  std: 24.0%\n",
      "FDR mean: 97.9%  std: 0.6%\n",
      "Epoch:      0, d_loss (Acc)): 0.012, v_loss (Acc): 0.009, g_loss: -1.0388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.44313861 8.13931796]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 97.5%  std: 15.6%\n",
      "FDR mean: 97.7%  std: 0.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.011, v_loss (Acc): 0.009, g_loss: -0.9505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.41454302 7.99841093]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.1%  std: 9.5%\n",
      "FDR mean: 97.4%  std: 0.6%\n",
      "Epoch:      0, d_loss (Acc)): 0.011, v_loss (Acc): 0.009, g_loss: -0.7440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.39631227 7.83912256]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 500 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.7%  std: 5.3%\n",
      "FDR mean: 97.1%  std: 0.7%\n",
      "Epoch:      0, d_loss (Acc)): 0.009, v_loss (Acc): 0.008, g_loss: -0.5647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.35783986 7.58800149]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 600 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 99.9%  std: 2.4%\n",
      "FDR mean: 96.5%  std: 1.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.008, v_loss (Acc): 0.007, g_loss: -0.3972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.30418536 7.24712875]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 700 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 1.4%\n",
      "FDR mean: 95.4%  std: 1.5%\n",
      "Epoch:      0, d_loss (Acc)): 0.008, v_loss (Acc): 0.007, g_loss: -0.1318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.28935764 6.79138312]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 800 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 93.3%  std: 2.9%\n",
      "Epoch:      0, d_loss (Acc)): 0.007, v_loss (Acc): 0.007, g_loss: +0.0161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.27526348 6.18415644]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 900 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 88.7%  std: 6.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.007, v_loss (Acc): 0.007, g_loss: +0.1496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.22954499 5.52440667]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1000 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 76.7%  std: 17.1%\n",
      "Epoch:      0, d_loss (Acc)): 0.005, v_loss (Acc): 0.006, g_loss: +0.3894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.17241244 5.09830179]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 50.4%  std: 30.7%\n",
      "Epoch:      0, d_loss (Acc)): 0.005, v_loss (Acc): 0.006, g_loss: +0.7830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.09839618 5.06373473]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 17.1%  std: 27.3%\n",
      "Epoch:      0, d_loss (Acc)): 0.004, v_loss (Acc): 0.005, g_loss: +1.1489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.01713505 4.67306138]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 2.6%  std: 12.2%\n",
      "Epoch:      0, d_loss (Acc)): 0.003, v_loss (Acc): 0.005, g_loss: +1.3254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.9219678  3.99472516]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.5%  std: 5.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.002, v_loss (Acc): 0.004, g_loss: +1.5693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.76725735 3.22283277]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1500 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 1.7%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.004, g_loss: +1.4486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.64074122 2.75117001]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1600 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.7%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.004, g_loss: +1.3076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.45187781 2.45381758]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1700 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +1.0591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.22659678 2.13217094]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1800 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +0.7993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.98073333 1.89995688]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 1900 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.003, g_loss: +0.6690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.77199477 1.69287985]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2000 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.002, g_loss: +0.5115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.61051574 1.5819502 ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2100 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.4239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.52728705 1.51857365]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2200 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.001, v_loss (Acc): 0.002, g_loss: +0.3256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.47611324 1.4471368 ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2300 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n",
      "Epoch:      0, d_loss (Acc)): 0.000, v_loss (Acc): 0.002, g_loss: +0.2481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.39301121 1.41759482]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "now at training epoch number 2400 hyp-params: lamda 0.1000 prior 0.0000\n",
      "rec time now 0 dataset now: Syn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPR mean: 100.0%  std: 0.0%\n",
      "FDR mean: 0.0%  std: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.31229983 1.3850124 ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n",
      "PyTorch Version: elapsed time for Syn1: 11 feature, 10000 sample: [252.9614] sec.\n",
      "Total T: 10200 Episode Num: 51 Episode T: 200 Reward: -1531.398\n",
      "Total T: 10400 Episode Num: 52 Episode T: 200 Reward: -1267.802\n",
      "Total T: 10600 Episode Num: 53 Episode T: 200 Reward: -1578.750\n",
      "Total T: 10800 Episode Num: 54 Episode T: 200 Reward: -1679.254\n",
      "Total T: 11000 Episode Num: 55 Episode T: 200 Reward: -1642.245\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1550.781\n",
      "---------------------------------------\n",
      "recent Evaluation: -1550.7805797382757\n",
      "Total T: 11200 Episode Num: 56 Episode T: 200 Reward: -1561.289\n",
      "Total T: 11400 Episode Num: 57 Episode T: 200 Reward: -1432.505\n",
      "Total T: 11600 Episode Num: 58 Episode T: 200 Reward: -1495.888\n",
      "Total T: 11800 Episode Num: 59 Episode T: 200 Reward: -1515.560\n",
      "Total T: 12000 Episode Num: 60 Episode T: 200 Reward: -1432.608\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1391.552\n",
      "---------------------------------------\n",
      "recent Evaluation: -1391.5519715866471\n",
      "Total T: 12200 Episode Num: 61 Episode T: 200 Reward: -1374.311\n",
      "Total T: 12400 Episode Num: 62 Episode T: 200 Reward: -1231.110\n",
      "Total T: 12600 Episode Num: 63 Episode T: 200 Reward: -990.598\n",
      "Total T: 12800 Episode Num: 64 Episode T: 200 Reward: -1051.381\n",
      "Total T: 13000 Episode Num: 65 Episode T: 200 Reward: -1518.286\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -882.691\n",
      "---------------------------------------\n",
      "recent Evaluation: -882.6911903689761\n",
      "Total T: 13200 Episode Num: 66 Episode T: 200 Reward: -1512.050\n",
      "Total T: 13400 Episode Num: 67 Episode T: 200 Reward: -887.858\n",
      "Total T: 13600 Episode Num: 68 Episode T: 200 Reward: -413.046\n",
      "Total T: 13800 Episode Num: 69 Episode T: 200 Reward: -475.235\n",
      "Total T: 14000 Episode Num: 70 Episode T: 200 Reward: -267.621\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -218.583\n",
      "---------------------------------------\n",
      "recent Evaluation: -218.58349755600165\n",
      "Total T: 14200 Episode Num: 71 Episode T: 200 Reward: -131.787\n",
      "Total T: 14400 Episode Num: 72 Episode T: 200 Reward: -411.680\n",
      "Total T: 14600 Episode Num: 73 Episode T: 200 Reward: -131.635\n",
      "Total T: 14800 Episode Num: 74 Episode T: 200 Reward: -386.069\n",
      "Total T: 15000 Episode Num: 75 Episode T: 200 Reward: -1515.269\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -202.677\n",
      "---------------------------------------\n",
      "recent Evaluation: -202.67684462998375\n",
      "Total T: 15200 Episode Num: 76 Episode T: 200 Reward: -127.635\n",
      "Total T: 15400 Episode Num: 77 Episode T: 200 Reward: -125.767\n",
      "Total T: 15600 Episode Num: 78 Episode T: 200 Reward: -0.628\n",
      "Total T: 15800 Episode Num: 79 Episode T: 200 Reward: -128.505\n",
      "Total T: 16000 Episode Num: 80 Episode T: 200 Reward: -130.592\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -111.659\n",
      "---------------------------------------\n",
      "recent Evaluation: -111.65894154668047\n",
      "Total T: 16200 Episode Num: 81 Episode T: 200 Reward: -3.491\n",
      "Total T: 16400 Episode Num: 82 Episode T: 200 Reward: -904.787\n",
      "Total T: 16600 Episode Num: 83 Episode T: 200 Reward: -129.430\n",
      "Total T: 16800 Episode Num: 84 Episode T: 200 Reward: -382.446\n",
      "Total T: 17000 Episode Num: 85 Episode T: 200 Reward: -135.226\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -146.621\n",
      "---------------------------------------\n",
      "recent Evaluation: -146.62075871306348\n",
      "Total T: 17200 Episode Num: 86 Episode T: 200 Reward: -127.868\n",
      "Total T: 17400 Episode Num: 87 Episode T: 200 Reward: -235.054\n",
      "Total T: 17600 Episode Num: 88 Episode T: 200 Reward: -3.490\n",
      "Total T: 17800 Episode Num: 89 Episode T: 200 Reward: -121.224\n",
      "Total T: 18000 Episode Num: 90 Episode T: 200 Reward: -131.825\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -124.078\n",
      "---------------------------------------\n",
      "recent Evaluation: -124.07778142199695\n",
      "Total T: 18200 Episode Num: 91 Episode T: 200 Reward: -247.794\n",
      "Total T: 18400 Episode Num: 92 Episode T: 200 Reward: -231.316\n",
      "Total T: 18600 Episode Num: 93 Episode T: 200 Reward: -126.885\n",
      "Total T: 18800 Episode Num: 94 Episode T: 200 Reward: -242.558\n",
      "Total T: 19000 Episode Num: 95 Episode T: 200 Reward: -127.032\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -187.962\n",
      "---------------------------------------\n",
      "recent Evaluation: -187.96236432725112\n",
      "Total T: 19200 Episode Num: 96 Episode T: 200 Reward: -364.282\n",
      "Total T: 19400 Episode Num: 97 Episode T: 200 Reward: -124.779\n",
      "Total T: 19600 Episode Num: 98 Episode T: 200 Reward: -245.679\n",
      "Total T: 19800 Episode Num: 99 Episode T: 200 Reward: -123.434\n",
      "Total T: 20000 Episode Num: 100 Episode T: 200 Reward: -115.552\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -124.763\n",
      "---------------------------------------\n",
      "recent Evaluation: -124.76268164982864\n",
      "Total T: 20200 Episode Num: 101 Episode T: 200 Reward: -117.717\n",
      "Total T: 20400 Episode Num: 102 Episode T: 200 Reward: -128.420\n",
      "Total T: 20600 Episode Num: 103 Episode T: 200 Reward: -117.564\n",
      "Total T: 20800 Episode Num: 104 Episode T: 200 Reward: -230.743\n",
      "Total T: 21000 Episode Num: 105 Episode T: 200 Reward: -338.486\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -135.168\n",
      "---------------------------------------\n",
      "recent Evaluation: -135.16750597990054\n",
      "Total T: 21200 Episode Num: 106 Episode T: 200 Reward: -116.991\n",
      "Total T: 21400 Episode Num: 107 Episode T: 200 Reward: -122.256\n",
      "Total T: 21600 Episode Num: 108 Episode T: 200 Reward: -118.012\n",
      "Total T: 21800 Episode Num: 109 Episode T: 200 Reward: -1.345\n",
      "Total T: 22000 Episode Num: 110 Episode T: 200 Reward: -117.738\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -160.110\n",
      "---------------------------------------\n",
      "recent Evaluation: -160.11018649179113\n",
      "Total T: 22200 Episode Num: 111 Episode T: 200 Reward: -4.894\n",
      "Total T: 22400 Episode Num: 112 Episode T: 200 Reward: -118.962\n",
      "Total T: 22600 Episode Num: 113 Episode T: 200 Reward: -319.347\n",
      "Total T: 22800 Episode Num: 114 Episode T: 200 Reward: -117.608\n",
      "Total T: 23000 Episode Num: 115 Episode T: 200 Reward: -229.988\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -126.867\n",
      "---------------------------------------\n",
      "recent Evaluation: -126.86654822838287\n",
      "Total T: 23200 Episode Num: 116 Episode T: 200 Reward: -239.326\n",
      "Total T: 23400 Episode Num: 117 Episode T: 200 Reward: -237.947\n",
      "Total T: 23600 Episode Num: 118 Episode T: 200 Reward: -232.726\n",
      "Total T: 23800 Episode Num: 119 Episode T: 200 Reward: -1.181\n",
      "Total T: 24000 Episode Num: 120 Episode T: 200 Reward: -117.357\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -166.078\n",
      "---------------------------------------\n",
      "recent Evaluation: -166.07773393360992\n",
      "Total T: 24200 Episode Num: 121 Episode T: 200 Reward: -114.592\n",
      "Total T: 24400 Episode Num: 122 Episode T: 200 Reward: -334.817\n",
      "Total T: 24600 Episode Num: 123 Episode T: 200 Reward: -1.075\n",
      "Total T: 24800 Episode Num: 124 Episode T: 200 Reward: -121.173\n",
      "Total T: 25000 Episode Num: 125 Episode T: 200 Reward: -123.872\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -143.249\n",
      "---------------------------------------\n",
      "recent Evaluation: -143.2485387962054\n",
      "Total T: 25200 Episode Num: 126 Episode T: 200 Reward: -120.468\n",
      "Total T: 25400 Episode Num: 127 Episode T: 200 Reward: -251.640\n",
      "Total T: 25600 Episode Num: 128 Episode T: 200 Reward: -120.347\n",
      "Total T: 25800 Episode Num: 129 Episode T: 200 Reward: -238.524\n",
      "Total T: 26000 Episode Num: 130 Episode T: 200 Reward: -124.567\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -175.983\n",
      "---------------------------------------\n",
      "recent Evaluation: -175.98343139121053\n",
      "Total T: 26200 Episode Num: 131 Episode T: 200 Reward: -231.573\n",
      "Total T: 26400 Episode Num: 132 Episode T: 200 Reward: -227.005\n",
      "Total T: 26600 Episode Num: 133 Episode T: 200 Reward: -118.255\n",
      "Total T: 26800 Episode Num: 134 Episode T: 200 Reward: -124.662\n",
      "Total T: 27000 Episode Num: 135 Episode T: 200 Reward: -122.259\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -133.309\n",
      "---------------------------------------\n",
      "recent Evaluation: -133.30890344494566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 27200 Episode Num: 136 Episode T: 200 Reward: -2.186\n",
      "Total T: 27400 Episode Num: 137 Episode T: 200 Reward: -1.057\n",
      "Total T: 27600 Episode Num: 138 Episode T: 200 Reward: -122.152\n",
      "Total T: 27800 Episode Num: 139 Episode T: 200 Reward: -234.413\n",
      "Total T: 28000 Episode Num: 140 Episode T: 200 Reward: -114.183\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.806\n",
      "---------------------------------------\n",
      "recent Evaluation: -142.806257752745\n",
      "Total T: 28200 Episode Num: 141 Episode T: 200 Reward: -119.651\n",
      "Total T: 28400 Episode Num: 142 Episode T: 200 Reward: -303.884\n",
      "Total T: 28600 Episode Num: 143 Episode T: 200 Reward: -1.967\n",
      "Total T: 28800 Episode Num: 144 Episode T: 200 Reward: -115.118\n",
      "Total T: 29000 Episode Num: 145 Episode T: 200 Reward: -238.982\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.078\n",
      "---------------------------------------\n",
      "recent Evaluation: -142.07790595400053\n",
      "Total T: 29200 Episode Num: 146 Episode T: 200 Reward: -125.038\n",
      "Total T: 29400 Episode Num: 147 Episode T: 200 Reward: -236.263\n",
      "Total T: 29600 Episode Num: 148 Episode T: 200 Reward: -120.291\n",
      "Total T: 29800 Episode Num: 149 Episode T: 200 Reward: -119.355\n",
      "Total T: 30000 Episode Num: 150 Episode T: 200 Reward: -2.819\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.967\n",
      "---------------------------------------\n",
      "recent Evaluation: -142.96678839675468\n",
      "Total T: 30200 Episode Num: 151 Episode T: 200 Reward: -229.790\n",
      "Total T: 30400 Episode Num: 152 Episode T: 200 Reward: -228.331\n",
      "Total T: 30600 Episode Num: 153 Episode T: 200 Reward: -117.560\n",
      "Total T: 30800 Episode Num: 154 Episode T: 200 Reward: -226.269\n",
      "Total T: 31000 Episode Num: 155 Episode T: 200 Reward: -236.962\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -143.865\n",
      "---------------------------------------\n",
      "recent Evaluation: -143.86460745027432\n",
      "Total T: 31200 Episode Num: 156 Episode T: 200 Reward: -226.354\n",
      "Total T: 31400 Episode Num: 157 Episode T: 200 Reward: -2.008\n",
      "Total T: 31600 Episode Num: 158 Episode T: 200 Reward: -228.376\n",
      "Total T: 31800 Episode Num: 159 Episode T: 200 Reward: -235.761\n",
      "Total T: 32000 Episode Num: 160 Episode T: 200 Reward: -240.137\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -108.049\n",
      "---------------------------------------\n",
      "recent Evaluation: -108.04912729408889\n",
      "Total T: 32200 Episode Num: 161 Episode T: 200 Reward: -238.177\n",
      "Total T: 32400 Episode Num: 162 Episode T: 200 Reward: -122.552\n",
      "Total T: 32600 Episode Num: 163 Episode T: 200 Reward: -123.401\n",
      "Total T: 32800 Episode Num: 164 Episode T: 200 Reward: -4.578\n",
      "Total T: 33000 Episode Num: 165 Episode T: 200 Reward: -116.827\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -143.741\n",
      "---------------------------------------\n",
      "recent Evaluation: -143.74105612212378\n",
      "Total T: 33200 Episode Num: 166 Episode T: 200 Reward: -3.012\n",
      "Total T: 33400 Episode Num: 167 Episode T: 200 Reward: -121.144\n",
      "Total T: 33600 Episode Num: 168 Episode T: 200 Reward: -124.464\n",
      "Total T: 33800 Episode Num: 169 Episode T: 200 Reward: -120.461\n",
      "Total T: 34000 Episode Num: 170 Episode T: 200 Reward: -1.294\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -113.718\n",
      "---------------------------------------\n",
      "recent Evaluation: -113.71759255768673\n",
      "Total T: 34200 Episode Num: 171 Episode T: 200 Reward: -114.592\n",
      "Total T: 34400 Episode Num: 172 Episode T: 200 Reward: -117.212\n",
      "Total T: 34600 Episode Num: 173 Episode T: 200 Reward: -332.450\n",
      "Total T: 34800 Episode Num: 174 Episode T: 200 Reward: -226.404\n",
      "Total T: 35000 Episode Num: 175 Episode T: 200 Reward: -122.830\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -109.734\n",
      "---------------------------------------\n",
      "recent Evaluation: -109.7338745894292\n",
      "Total T: 35200 Episode Num: 176 Episode T: 200 Reward: -120.170\n",
      "Total T: 35400 Episode Num: 177 Episode T: 200 Reward: -261.979\n",
      "Total T: 35600 Episode Num: 178 Episode T: 200 Reward: -2.554\n",
      "Total T: 35800 Episode Num: 179 Episode T: 200 Reward: -237.868\n",
      "Total T: 36000 Episode Num: 180 Episode T: 200 Reward: -124.211\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -106.135\n",
      "---------------------------------------\n",
      "recent Evaluation: -106.13462487788391\n",
      "Total T: 36200 Episode Num: 181 Episode T: 200 Reward: -122.975\n",
      "Total T: 36400 Episode Num: 182 Episode T: 200 Reward: -235.561\n",
      "Total T: 36600 Episode Num: 183 Episode T: 200 Reward: -127.707\n",
      "Total T: 36800 Episode Num: 184 Episode T: 200 Reward: -122.856\n",
      "Total T: 37000 Episode Num: 185 Episode T: 200 Reward: -116.189\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -184.585\n",
      "---------------------------------------\n",
      "recent Evaluation: -184.58513883711277\n",
      "Total T: 37200 Episode Num: 186 Episode T: 200 Reward: -125.348\n",
      "Total T: 37400 Episode Num: 187 Episode T: 200 Reward: -119.818\n",
      "Total T: 37600 Episode Num: 188 Episode T: 200 Reward: -126.991\n",
      "Total T: 37800 Episode Num: 189 Episode T: 200 Reward: -122.571\n",
      "Total T: 38000 Episode Num: 190 Episode T: 200 Reward: -234.490\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -189.794\n",
      "---------------------------------------\n",
      "recent Evaluation: -189.79374103966921\n",
      "Total T: 38200 Episode Num: 191 Episode T: 200 Reward: -127.043\n",
      "Total T: 38400 Episode Num: 192 Episode T: 200 Reward: -124.082\n",
      "Total T: 38600 Episode Num: 193 Episode T: 200 Reward: -125.928\n",
      "Total T: 38800 Episode Num: 194 Episode T: 200 Reward: -116.867\n",
      "Total T: 39000 Episode Num: 195 Episode T: 200 Reward: -323.017\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -164.904\n",
      "---------------------------------------\n",
      "recent Evaluation: -164.90417748050967\n",
      "Total T: 39200 Episode Num: 196 Episode T: 200 Reward: -117.250\n",
      "Total T: 39400 Episode Num: 197 Episode T: 200 Reward: -125.613\n",
      "Total T: 39600 Episode Num: 198 Episode T: 200 Reward: -226.692\n",
      "Total T: 39800 Episode Num: 199 Episode T: 200 Reward: -124.294\n",
      "Total T: 40000 Episode Num: 200 Episode T: 200 Reward: -120.942\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -96.230\n",
      "---------------------------------------\n",
      "recent Evaluation: -96.2304880519303\n",
      "Total T: 40200 Episode Num: 201 Episode T: 200 Reward: -230.705\n",
      "Total T: 40400 Episode Num: 202 Episode T: 200 Reward: -129.903\n",
      "Total T: 40600 Episode Num: 203 Episode T: 200 Reward: -224.672\n",
      "Total T: 40800 Episode Num: 204 Episode T: 200 Reward: -118.992\n",
      "Total T: 41000 Episode Num: 205 Episode T: 200 Reward: -127.235\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -131.506\n",
      "---------------------------------------\n",
      "recent Evaluation: -131.50614295139457\n",
      "Total T: 41200 Episode Num: 206 Episode T: 200 Reward: -127.952\n",
      "Total T: 41400 Episode Num: 207 Episode T: 200 Reward: -128.535\n",
      "Total T: 41600 Episode Num: 208 Episode T: 200 Reward: -124.271\n",
      "Total T: 41800 Episode Num: 209 Episode T: 200 Reward: -374.081\n",
      "Total T: 42000 Episode Num: 210 Episode T: 200 Reward: -117.843\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -140.176\n",
      "---------------------------------------\n",
      "recent Evaluation: -140.17636878694793\n",
      "Total T: 42200 Episode Num: 211 Episode T: 200 Reward: -118.960\n",
      "Total T: 42400 Episode Num: 212 Episode T: 200 Reward: -242.858\n",
      "Total T: 42600 Episode Num: 213 Episode T: 200 Reward: -230.201\n",
      "Total T: 42800 Episode Num: 214 Episode T: 200 Reward: -3.386\n",
      "Total T: 43000 Episode Num: 215 Episode T: 200 Reward: -127.404\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -109.747\n",
      "---------------------------------------\n",
      "recent Evaluation: -109.74718909440642\n",
      "Total T: 43200 Episode Num: 216 Episode T: 200 Reward: -1.689\n",
      "Total T: 43400 Episode Num: 217 Episode T: 200 Reward: -125.707\n",
      "Total T: 43600 Episode Num: 218 Episode T: 200 Reward: -226.496\n",
      "Total T: 43800 Episode Num: 219 Episode T: 200 Reward: -1.839\n",
      "Total T: 44000 Episode Num: 220 Episode T: 200 Reward: -2.346\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -126.821\n",
      "---------------------------------------\n",
      "recent Evaluation: -126.82116376176143\n",
      "Total T: 44200 Episode Num: 221 Episode T: 200 Reward: -227.331\n",
      "Total T: 44400 Episode Num: 222 Episode T: 200 Reward: -227.523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 44600 Episode Num: 223 Episode T: 200 Reward: -364.296\n",
      "Total T: 44800 Episode Num: 224 Episode T: 200 Reward: -116.867\n",
      "Total T: 45000 Episode Num: 225 Episode T: 200 Reward: -118.403\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.169\n",
      "---------------------------------------\n",
      "recent Evaluation: -132.1687466874569\n",
      "Total T: 45200 Episode Num: 226 Episode T: 200 Reward: -123.436\n",
      "Total T: 45400 Episode Num: 227 Episode T: 200 Reward: -1.821\n",
      "Total T: 45600 Episode Num: 228 Episode T: 200 Reward: -127.878\n",
      "Total T: 45800 Episode Num: 229 Episode T: 200 Reward: -225.010\n",
      "Total T: 46000 Episode Num: 230 Episode T: 200 Reward: -230.348\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -122.406\n",
      "---------------------------------------\n",
      "recent Evaluation: -122.40551292648338\n",
      "Total T: 46200 Episode Num: 231 Episode T: 200 Reward: -235.938\n",
      "Total T: 46400 Episode Num: 232 Episode T: 200 Reward: -117.960\n",
      "Total T: 46600 Episode Num: 233 Episode T: 200 Reward: -116.709\n",
      "Total T: 46800 Episode Num: 234 Episode T: 200 Reward: -127.758\n",
      "Total T: 47000 Episode Num: 235 Episode T: 200 Reward: -236.361\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -98.288\n",
      "---------------------------------------\n",
      "recent Evaluation: -98.28787161393981\n",
      "Total T: 47200 Episode Num: 236 Episode T: 200 Reward: -128.388\n",
      "Total T: 47400 Episode Num: 237 Episode T: 200 Reward: -230.470\n",
      "Total T: 47600 Episode Num: 238 Episode T: 200 Reward: -127.048\n",
      "Total T: 47800 Episode Num: 239 Episode T: 200 Reward: -2.407\n",
      "Total T: 48000 Episode Num: 240 Episode T: 200 Reward: -115.208\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -136.474\n",
      "---------------------------------------\n",
      "recent Evaluation: -136.47416017970082\n",
      "Total T: 48200 Episode Num: 241 Episode T: 200 Reward: -119.653\n",
      "Total T: 48400 Episode Num: 242 Episode T: 200 Reward: -130.461\n",
      "Total T: 48600 Episode Num: 243 Episode T: 200 Reward: -372.032\n",
      "Total T: 48800 Episode Num: 244 Episode T: 200 Reward: -119.977\n",
      "Total T: 49000 Episode Num: 245 Episode T: 200 Reward: -118.002\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -155.152\n",
      "---------------------------------------\n",
      "recent Evaluation: -155.15186326478675\n",
      "Total T: 49200 Episode Num: 246 Episode T: 200 Reward: -115.750\n",
      "Total T: 49400 Episode Num: 247 Episode T: 200 Reward: -223.618\n",
      "Total T: 49600 Episode Num: 248 Episode T: 200 Reward: -127.637\n",
      "Total T: 49800 Episode Num: 249 Episode T: 200 Reward: -114.257\n",
      "Total T: 50000 Episode Num: 250 Episode T: 200 Reward: -0.422\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -131.989\n",
      "---------------------------------------\n",
      "recent Evaluation: -131.98896255858944\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'Pendulum-v0'\n",
    "alias = 'Fixed_INVASE'\n",
    "RED_ACTION_DIM = 100\n",
    "import gym\n",
    "print('\\n now evaluating: \\n       ', ENV_NAME)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import TD3_INVASE\n",
    "\n",
    "def eval_policy(policy, eval_episodes=10):\n",
    "    eval_env = gym.make(ENV_NAME)\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, _ = eval_env.step(action[:-RED_ACTION_DIM])\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "#spec = env.action_space\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] + RED_ACTION_DIM\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "args_policy_noise = 0.2\n",
    "args_noise_clip = 0.5\n",
    "args_policy_freq = 2\n",
    "args_max_timesteps = 10000\n",
    "args_expl_noise = 0.1\n",
    "args_batch_size = 256\n",
    "args_eval_freq = 1000\n",
    "args_start_timesteps = 10000\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": 0.99,\n",
    "    \"tau\": 0.005\n",
    "}\n",
    "\n",
    "for repeat in range(5):\n",
    "    kwargs[\"policy_noise\"] = args_policy_noise * max_action\n",
    "    kwargs[\"noise_clip\"] = args_noise_clip * max_action\n",
    "    kwargs[\"policy_freq\"] = args_policy_freq\n",
    "    policy = TD3_INVASE.TD3(**kwargs)\n",
    "    replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "    # Evaluate untrained policy\n",
    "    evaluations = [eval_policy(policy)]\n",
    "    \n",
    "    state, done = env.reset(), False\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "    counter = 0\n",
    "    msk_list = []        \n",
    "    temp_curve = [eval_policy(policy)]\n",
    "    temp_val = []\n",
    "    for t in range(int(args_max_timesteps)):\n",
    "        episode_timesteps += 1\n",
    "        counter += 1\n",
    "        # Select action randomly or according to policy\n",
    "        if t < args_start_timesteps:\n",
    "            action = np.random.uniform(-max_action, max_action, action_dim)\n",
    "        else:\n",
    "            if np.random.uniform(0,1) < 0.0:\n",
    "                action = np.random.uniform(-max_action, max_action, action_dim)\n",
    "            else:\n",
    "                action = (\n",
    "                    policy.select_action(np.array(state))\n",
    "                    + np.random.normal(0, max_action * args_expl_noise, size=action_dim)\n",
    "                ).clip(-max_action, max_action)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action[:-RED_ACTION_DIM])\n",
    "        \n",
    "\n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "        replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if t >= args_start_timesteps:\n",
    "            '''TD3'''\n",
    "            policy.train(replay_buffer, args_batch_size)\n",
    "                    \n",
    "                    \n",
    "        # Train agent after collecting sufficient data\n",
    "        if done:\n",
    "            print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "            msk_list = []\n",
    "            state, done = env.reset(), False\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1 \n",
    "\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % args_eval_freq == 0:\n",
    "            evaluations.append(eval_policy(policy))\n",
    "            print('recent Evaluation:',evaluations[-1])\n",
    "            np.save('results/evaluations_alias{}_ENV{}_Repeat{}'.format(alias,ENV_NAME,repeat),evaluations)\n",
    "            \n",
    "            \n",
    "    state_list_train = replay_buffer.state[:args_start_timesteps-5000]\n",
    "    state_list_test = replay_buffer.state[args_start_timesteps-5000:args_start_timesteps]\n",
    "\n",
    "    action_list_train = replay_buffer.action[:args_start_timesteps-5000]\n",
    "    action_list_test = replay_buffer.action[args_start_timesteps-5000:args_start_timesteps]\n",
    "    next_state_list_train = replay_buffer.next_state[:args_start_timesteps-5000]\n",
    "    next_state_list_test = replay_buffer.next_state[args_start_timesteps-5000:args_start_timesteps]\n",
    "\n",
    "    state_delta_train = next_state_list_train - state_list_train\n",
    "    state_delta_test = next_state_list_test - state_list_test\n",
    "            \n",
    "            \n",
    "    X_DIM = action_list_train.shape[1] # feature dimension Hyper-Param\n",
    "    \n",
    "    \n",
    "    class init_arg(object):\n",
    "        def __init__(self, it = 10000, o = 'feature_score.csv.gz', dataset = None, i= None, target = None):\n",
    "            self.it = it\n",
    "            self.o = o\n",
    "            self.dataset = dataset\n",
    "            self.i = i\n",
    "            self.target = target\n",
    "\n",
    "    args = init_arg(dataset = 'Syn5', it = 300, )\n",
    "    ocsv = args.o # 'feature_score.csv.gz'\n",
    "    odir = os.path.dirname(ocsv)\n",
    "    odir = '.' if not len(odir) else odir\n",
    "    fn_csv = args.i #'data.csv'\n",
    "    label_nm = args.target # 'target'\n",
    "    nepoch = args.it\n",
    "    logger = utilmlab.init_logger(odir)\n",
    "\n",
    "    dataset = args.dataset\n",
    "\n",
    "    assert dataset is not None or fn_csv is not None\n",
    "    assert fn_csv is None or label_nm is not None\n",
    "\n",
    "    # Data output can be either binary (Y) or Probability (Prob)\n",
    "    data_out_sets = ['Y', 'Prob']\n",
    "    data_out = data_out_sets[0]\n",
    "\n",
    "    logger.info('invase: {} {} {} {}'.format(dataset, nepoch, odir, data_out))\n",
    "\n",
    "    # Number of Training and Testing samples\n",
    "    train_N = 10000\n",
    "    test_N = 10000\n",
    "\n",
    "    # Seeds (different seeds for training and testing)\n",
    "    train_seed = 0\n",
    "    test_seed = 1\n",
    "\n",
    "    xs_train,xa_train, y_train, xs_test, xa_test, y_test= state_list_train, action_list_train, state_delta_train, state_list_test, action_list_test, state_delta_test, \n",
    "    g_test = np.zeros((y_test.shape[0],RED_ACTION_DIM + env.action_space.shape[0]))\n",
    "    g_test[:,0] = 1\n",
    "    print(g_test)\n",
    "    print(xs_train.shape, xa_train.shape, y_train.shape, xs_test.shape, xa_test.shape, y_test.shape, g_test.shape)\n",
    "            \n",
    "    '''learning INVASE'''\n",
    "    \n",
    "    REAL_LMD = 1.0 # 0.0 - 0.5\n",
    "\n",
    "\n",
    "    import time\n",
    "    elapsed_time = []\n",
    "\n",
    "    class init_arg(object):\n",
    "        def __init__(self, it = 10000, o = 'feature_score.csv.gz', dataset = None, i= None, target = None):\n",
    "            self.it = it\n",
    "            self.o = o\n",
    "            self.dataset = dataset\n",
    "            self.i = i\n",
    "            self.target = target\n",
    "\n",
    "\n",
    "    for DATASET in ['Syn1']:\n",
    "        args = init_arg(dataset = DATASET, it = 2500,)\n",
    "        ocsv = args.o # 'feature_score.csv.gz'\n",
    "        odir = os.path.dirname(ocsv)\n",
    "        odir = '.' if not len(odir) else odir\n",
    "        fn_csv = args.i #'data.csv'\n",
    "        label_nm = args.target # 'target'\n",
    "        nepoch = args.it\n",
    "        logger = utilmlab.init_logger(odir)\n",
    "\n",
    "        dataset = args.dataset\n",
    "\n",
    "        assert dataset is not None or fn_csv is not None\n",
    "        assert fn_csv is None or label_nm is not None\n",
    "\n",
    "        # Data output can be either binary (Y) or Probability (Prob)\n",
    "        data_out_sets = ['Y', 'Prob']\n",
    "        data_out = data_out_sets[0]\n",
    "\n",
    "        logger.info('invase: {} {} {} {}'.format(dataset, nepoch, odir, data_out))\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        for thres_i in [0.0]:\n",
    "            Predict_Out_temp = np.zeros([3, 2])    \n",
    "\n",
    "            PVS_Alg = PVS(xs_train, dataset, 100, thres=thres_i)\n",
    "\n",
    "            print('start training......')\n",
    "\n",
    "            for train_epoch in range(int(nepoch/100)):\n",
    "\n",
    "                Lmd = 0.1 #train_epoch*100/nepoch * REAL_LMD\n",
    "                Thr = 0.0 #0.5*(1 - train_epoch*100/nepoch)\n",
    "                print('now at training epoch number', int(train_epoch * 100),'hyp-params: lamda %.4f prior %.4f'%(Lmd,Thr))\n",
    "                PVS_Alg.train(xs_train, xa_train, y_train, lmd = Lmd , thr = Thr)\n",
    "                # 3. Get the selection probability on the testing set\n",
    "                #Sel_Prob_Test = PVS_Alg.output(x_test)\n",
    "\n",
    "\n",
    "\n",
    "                '''recurssive generation'''\n",
    "                input_batch_xs = xs_test * 1.0\n",
    "                input_batch_xa = xa_test * 1.0\n",
    "\n",
    "                sel_prob_tot = 1.0\n",
    "                for recur_time in range(1):\n",
    "                    print('rec time now',recur_time,'dataset now:',DATASET)\n",
    "                    gen_prob = PVS_Alg.generator(torch.as_tensor(input_batch_xs).float(),torch.as_tensor(input_batch_xa).float())\n",
    "                    #sel_prob = PVS_Alg.Sample_M(gen_prob)\n",
    "                    sel_prob = 1.*(gen_prob > 0.5)\n",
    "                    sel_prob_tot_0 = sel_prob_tot * 1.0\n",
    "                    sel_prob_tot = sel_prob * sel_prob_tot\n",
    "                    input_batch_xa = sel_prob_tot * input_batch_xa\n",
    "\n",
    "                    score = sel_prob_tot\n",
    "                    #print('score',score)\n",
    "\n",
    "\n",
    "\n",
    "                    # 4. Selected features\n",
    "                    # 5. Prediction\n",
    "                    val_predict, dis_predict = PVS_Alg.get_prediction(torch.as_tensor(xs_test).float(),torch.as_tensor(xa_test).float(), score)\n",
    "\n",
    "                    def performance_metric(score, g_truth):\n",
    "\n",
    "                        n = len(score)\n",
    "                        Temp_TPR = np.zeros([n,])\n",
    "                        Temp_FDR = np.zeros([n,])\n",
    "\n",
    "                        for i in range(n):\n",
    "\n",
    "                            # TPR    \n",
    "                            # embed()\n",
    "                            TPR_Nom = np.sum((score[i,:] * g_truth[i,:]).cpu().detach().numpy())\n",
    "                            TPR_Den = np.sum(g_truth[i,:])\n",
    "                            Temp_TPR[i] = 100 * float(TPR_Nom)/float(TPR_Den+1e-8)\n",
    "\n",
    "                            # FDR\n",
    "                            FDR_Nom = np.sum((score[i,:] * (1-g_truth[i,:])).cpu().detach().numpy())\n",
    "                            FDR_Den = np.sum(score[i,:].cpu().detach().numpy())\n",
    "                            Temp_FDR[i] = 100 * float(FDR_Nom)/float(FDR_Den+1e-8)\n",
    "\n",
    "                        return np.mean(Temp_TPR), np.mean(Temp_FDR),\\\n",
    "                            np.std(Temp_TPR), np.std(Temp_FDR)\n",
    "\n",
    "                    #%% Output\n",
    "\n",
    "                    TPR_mean, TPR_std = -1, 0\n",
    "                    FDR_mean, FDR_std = -1, 0\n",
    "                    if g_test is not None:\n",
    "                        TPR_mean, FDR_mean, TPR_std, FDR_std = performance_metric(\n",
    "                            score, g_test)\n",
    "\n",
    "                        logger.info('TPR mean: {:0.1f}%  std: {:0.1f}%'.format(\n",
    "                            TPR_mean, TPR_std))\n",
    "                        logger.info('FDR mean: {:0.1f}%  std: {:0.1f}%'.format(\n",
    "                            FDR_mean, FDR_std))\n",
    "                    else:\n",
    "                        logger.info('no ground truth relevance')\n",
    "\n",
    "\n",
    "\n",
    "                    #%% Performance Metrics\n",
    "                    Predict_Out_temp[0,0] = np.linalg.norm(y_test - val_predict,2).mean()\n",
    "                    Predict_Out_temp[0,1] = np.linalg.norm(y_test - dis_predict,2).mean()\n",
    "                    print(Predict_Out_temp)\n",
    "\n",
    "        elapsed_time.append(time.time() - start_time)\n",
    "        print('PyTorch Version: elapsed time for {}: 11 feature, 10000 sample:'.format(DATASET),np.round(elapsed_time,4),'sec.')\n",
    "\n",
    "\n",
    "    '''Continue training with fixed INVASE model'''\n",
    "    PVS_Alg.generator.cuda()\n",
    "    PVS_Alg.generator.eval()\n",
    "    for t in range(10000, 50000):\n",
    "        episode_timesteps += 1\n",
    "        counter += 1\n",
    "        # Select action randomly or according to policy\n",
    "        if t < args_start_timesteps:\n",
    "            action = np.random.uniform(-max_action, max_action, action_dim)\n",
    "        else:\n",
    "            if np.random.uniform(0,1) < 0.0:\n",
    "                action = np.random.uniform(-max_action, max_action, action_dim)\n",
    "            else:\n",
    "                action = (\n",
    "                    policy.select_action(np.array(state))\n",
    "                    + np.random.normal(0, max_action * args_expl_noise, size=action_dim)\n",
    "                ).clip(-max_action, max_action)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action[:-RED_ACTION_DIM])\n",
    "\n",
    "\n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "        replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if t >= args_start_timesteps:\n",
    "            '''TD3'''\n",
    "            policy.train(replay_buffer, args_batch_size, PVS_Alg)\n",
    "\n",
    "\n",
    "        # Train agent after collecting sufficient data\n",
    "        if done:\n",
    "            print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "            msk_list = []\n",
    "            state, done = env.reset(), False\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1 \n",
    "\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % args_eval_freq == 0:\n",
    "            evaluations.append(eval_policy(policy))\n",
    "            print('recent Evaluation:',evaluations[-1])\n",
    "            np.save('results/evaluations_alias{}_ENV{}_Repeat{}'.format(alias,ENV_NAME,repeat),evaluations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.79",
   "language": "python",
   "name": "py2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
